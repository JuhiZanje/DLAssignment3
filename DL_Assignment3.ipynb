{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"026718ca95cb4435802143337cd8b70b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7cb187525684de6b61ffb01c934626c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3bd0841df2e14a05b8a687578134272c","value":1}},"0499e51a615043cc9da4f5c47dedf592":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c67069c5834d48ea8e58ae901c111615","IPY_MODEL_0b33d8b3437046cbba0b33a75f0702be","IPY_MODEL_5164305beb4945a7b6f67f60b28b161f"],"layout":"IPY_MODEL_e11eb1abd3614bcba0776962fa536be5"}},"0b33d8b3437046cbba0b33a75f0702be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_76f860bac31848088056ad2f41efd906","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90363bd470494db7b33a89ab8693cf26","value":1}},"230cc3dd6a394d90bf1c0982b4928f89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b47694ffe1740bfb4afc89b3aec8b83":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bd0841df2e14a05b8a687578134272c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fc597a1bb7c4badb1db05e2674b4cc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43ca68c849b14af19f87274e8b9b00e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7997debfbaeb45009a2bf4f2e7e487a8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f50298c14b243409a0f54e2fb6588a7","value":1}},"4f50298c14b243409a0f54e2fb6588a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5164305beb4945a7b6f67f60b28b161f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e98b49124f5456d8be4c101474aea87","placeholder":"​","style":"IPY_MODEL_e0b8f94992eb47e397d14f82d2e89e90","value":" 440/? [05:07&lt;00:00,  1.43it/s]"}},"5e98b49124f5456d8be4c101474aea87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75f1bae28b104aa3a6b8376ce6e67b16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76f860bac31848088056ad2f41efd906":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7997debfbaeb45009a2bf4f2e7e487a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f9a0235399d4985a8363fc799a7ea24":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90363bd470494db7b33a89ab8693cf26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c67069c5834d48ea8e58ae901c111615":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b47694ffe1740bfb4afc89b3aec8b83","placeholder":"​","style":"IPY_MODEL_75f1bae28b104aa3a6b8376ce6e67b16","value":"Testing: "}},"c7cb187525684de6b61ffb01c934626c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d40b59a3ad5d42ae9c4dde8c415d1eb5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d52f6ff85f5644e5bd73feebb3f1300f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0b8f94992eb47e397d14f82d2e89e90":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e11eb1abd3614bcba0776962fa536be5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e2c6e0ffa3594104a22359e77e3ab090":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_f9c6934d3cb244f7b20f46ca19a2c7c5","IPY_MODEL_026718ca95cb4435802143337cd8b70b"],"layout":"IPY_MODEL_d52f6ff85f5644e5bd73feebb3f1300f"}},"e90f9274ac9545c3ae5459fea17fbe32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ede0515ca11f420f9d2c04593d256113":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ef35712b7ef745a0b3e7d8cfb0d91bc3","IPY_MODEL_43ca68c849b14af19f87274e8b9b00e0"],"layout":"IPY_MODEL_d40b59a3ad5d42ae9c4dde8c415d1eb5"}},"ef35712b7ef745a0b3e7d8cfb0d91bc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e90f9274ac9545c3ae5459fea17fbe32","placeholder":"​","style":"IPY_MODEL_230cc3dd6a394d90bf1c0982b4928f89","value":"0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"}},"f9c6934d3cb244f7b20f46ca19a2c7c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f9a0235399d4985a8363fc799a7ea24","placeholder":"​","style":"IPY_MODEL_3fc597a1bb7c4badb1db05e2674b4cc9","value":"0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8190591,"sourceType":"datasetVersion","datasetId":4850432},{"sourceId":8304151,"sourceType":"datasetVersion","datasetId":4933099},{"sourceId":8304593,"sourceType":"datasetVersion","datasetId":4933272}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{"id":"VejTsKhNLm2A"}},{"cell_type":"code","source":"!pip install pytorch_lightning\n!pip install wandb\nimport torch\ntorch.cuda.empty_cache()\nimport torch.utils.data as data\nimport zipfile\nimport wandb\nimport warnings\nwarnings.filterwarnings(\"ignore\") \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mUM6KgAM-oT1","outputId":"e148e589-ad2a-40a1-df9e-9301dad04358","execution":{"iopub.status.busy":"2024-05-06T08:43:36.615297Z","iopub.execute_input":"2024-05-06T08:43:36.615958Z","iopub.status.idle":"2024-05-06T08:44:08.878333Z","shell.execute_reply.started":"2024-05-06T08:43:36.615925Z","shell.execute_reply":"2024-05-06T08:44:08.877409Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.26.4)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (2.1.2)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.66.1)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (6.0.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.2.0)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.3.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (21.3)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.9.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (0.11.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch_lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch_lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->pytorch_lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->pytorch_lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{"id":"HwQHFp9POlu8"}},{"cell_type":"markdown","source":"###Mounting Drive\n\n\n","metadata":{"id":"tOCLaWWqNtN9"}},{"cell_type":"code","source":"# # Mount Google Drive to access the dataset\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pEgf2dIi-oUB","outputId":"ad859be8-e2e8-4df1-b0a2-8aa543c21772","execution":{"iopub.status.busy":"2024-05-06T08:44:08.880339Z","iopub.execute_input":"2024-05-06T08:44:08.880965Z","iopub.status.idle":"2024-05-06T08:44:08.886463Z","shell.execute_reply.started":"2024-05-06T08:44:08.880929Z","shell.execute_reply":"2024-05-06T08:44:08.885406Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# # Set the path to your zipped dataset\n# zip_file_path = '/content/drive/MyDrive/DL /aksharantar_sampled.zip'\n\n# # Extract the dataset to a folder in Google Drive\n# zip_ref = zipfile.ZipFile(zip_file_path, 'r')\n# zip_ref.extractall('aksharantar_sampled_extracted')\n# zip_ref.close()","metadata":{"id":"kFtwXvrn-oUC","execution":{"iopub.status.busy":"2024-05-06T08:44:08.887732Z","iopub.execute_input":"2024-05-06T08:44:08.888073Z","iopub.status.idle":"2024-05-06T08:44:08.894812Z","shell.execute_reply.started":"2024-05-06T08:44:08.888040Z","shell.execute_reply":"2024-05-06T08:44:08.893847Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# from matplotlib.font_manager import FontProperties\n# import seaborn as sns\n# # !unzip Tiro_Devanagari_Hindi.zip -d hindi\n# # hindi_font = FontProperties(fname = '/kaggle/input/tiro-devanagari-hindi/TiroDevanagariHindi-Regular.ttf')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4sUEZbyOJMQ","outputId":"cd39e823-cc3d-4340-d044-0fbd89ea3bb7","execution":{"iopub.status.busy":"2024-05-06T08:44:08.896974Z","iopub.execute_input":"2024-05-06T08:44:08.897250Z","iopub.status.idle":"2024-05-06T08:44:08.903372Z","shell.execute_reply.started":"2024-05-06T08:44:08.897217Z","shell.execute_reply":"2024-05-06T08:44:08.902441Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n# from google.colab import drive\nimport csv\n# move tensors to GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Path to your CSV file on Google Drive (Extracted file)\ncsv_file_path = '/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv'\n\n# Read the CSV file and extract the sequence of characters\nwith open(csv_file_path, 'r') as f:\n    reader = csv.reader(f)\n    chars = []\n    for row in reader:\n        chars.extend(row[0])  # assuming the first column of the CSV file contains the text data\n\ncharS=set(chars)\ncharS.add('|')\nchar_set = list(charS)\n\n# Define the mapping between characters and integer indices\nchar_to_idx_latin= {char: i+1 for i, char in enumerate(char_set)}\n","metadata":{"id":"R-OebR_K-oUE","execution":{"iopub.status.busy":"2024-05-06T08:44:08.904783Z","iopub.execute_input":"2024-05-06T08:44:08.905267Z","iopub.status.idle":"2024-05-06T08:44:09.056082Z","shell.execute_reply.started":"2024-05-06T08:44:08.905210Z","shell.execute_reply":"2024-05-06T08:44:09.054761Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"max_length_devanagari=0\nwith open(csv_file_path, 'r') as f:\n    reader = csv.reader(f)\n    chars = []\n\n    for row in reader:\n        chars.extend(row[1])  # assuming the first column of the CSV file contains the text data\n\n# Define the character set\ncharS=set(chars)\ncharS.add('|')\nchar_set = list(charS)\n\n# Define the mapping between characters and integer indices\nchar_to_idx_lang ={char: i+1 for i, char in enumerate(char_set)}","metadata":{"id":"BsSPFp2f-oUF","execution":{"iopub.status.busy":"2024-05-06T08:44:09.057846Z","iopub.execute_input":"2024-05-06T08:44:09.058789Z","iopub.status.idle":"2024-05-06T08:44:09.169714Z","shell.execute_reply.started":"2024-05-06T08:44:09.058739Z","shell.execute_reply":"2024-05-06T08:44:09.168818Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"max_length_latin = 0\n\n# Read the CSV file and extract the sequence of characters\nwith open(csv_file_path, 'r') as f:\n    reader = csv.reader(f)\n    chars = []\n\n    for row in reader:\n        length = len(row[0])  # assuming the column you want to check is the first one\n        if length > max_length_latin:\n            max_length_latin = length","metadata":{"id":"YnH-KUwf-oUH","execution":{"iopub.status.busy":"2024-05-06T08:44:09.171026Z","iopub.execute_input":"2024-05-06T08:44:09.171362Z","iopub.status.idle":"2024-05-06T08:44:09.238717Z","shell.execute_reply.started":"2024-05-06T08:44:09.171334Z","shell.execute_reply":"2024-05-06T08:44:09.237563Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"max_length_devanagari = 0\n# Read the CSV file and extract the sequence of characters\nwith open(csv_file_path, 'r') as f:\n    reader = csv.reader(f)\n    chars = []\n\n    for row in reader:\n        length = len(row[1])  # assuming the column you want to check is the first one\n        if length > max_length_devanagari:\n            max_length_devanagari = length","metadata":{"id":"_CT0LTQL-oUI","execution":{"iopub.status.busy":"2024-05-06T08:44:09.240439Z","iopub.execute_input":"2024-05-06T08:44:09.241392Z","iopub.status.idle":"2024-05-06T08:44:09.305143Z","shell.execute_reply.started":"2024-05-06T08:44:09.241359Z","shell.execute_reply":"2024-05-06T08:44:09.304036Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Converting characters in words to indices\n\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"j98Htn0kQTdn"}},{"cell_type":"code","source":"def word_to_indices(word, max_length,dict):\n    # Convert the characters to integer indices using the char_to_idx mapping\n    indices = [dict.get(c, -1) for c in word]\n    # Filter out characters not in the dictionary\n    indices = [idx for idx in indices if idx >= 0]\n    # Add padding if necessary to make the sequence length equal to max_length\n    if len(indices) < max_length:\n        indices += [0] * (max_length - len(indices))\n    # Truncate the sequence if necessary to make the sequence length equal to max_length\n    elif len(indices) > max_length:\n        indices = indices[:max_length]\n    # Add the start and end tokens to the sequence\n    indices = [dict['|']] + indices + [dict['|']]\n    # Convert the list of indices to a tensor\n    tensor = torch.tensor(indices)\n    tensor=tensor.to(device)\n    return tensor\n\npairs=[]\n# Read the CSV file containing Latin-Devanagari word pairs\nwith open('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv', 'r') as f:\n    reader = csv.reader(f)\n    for row in reader:\n        latin_word = row[0]\n        devanagari_word = row[1]\n        # Convert the Latin word to a tensor of integer indices\n        latin_indices = word_to_indices(latin_word, max_length_latin,char_to_idx_latin)\n        devanagari_indices= word_to_indices(devanagari_word,max_length_devanagari ,char_to_idx_lang)\n        pairs.append([latin_indices,devanagari_indices])\n\npairs_v=[]\n# Read the CSV file containing Latin-Devanagari word pairs\nwith open('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_valid.csv', 'r') as f_v:\n    reader_v = csv.reader(f_v)\n    for row in reader_v:\n        latin_word = row[0]\n        devanagari_word = row[1]\n        # Convert the Latin word to a tensor of integer indices\n        latin_indices = word_to_indices(latin_word, max_length_latin,char_to_idx_latin)\n        devanagari_indices= word_to_indices(devanagari_word,max_length_devanagari ,char_to_idx_lang)\n        pairs_v.append([latin_indices,devanagari_indices])\n\npairs_t=[]\n# Read the CSV file containing Latin-Devanagari word pairs\nwith open('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_test.csv', 'r') as f_t:\n    reader_t = csv.reader(f_t)\n    for row in reader_t:\n        latin_word = row[0]\n        devanagari_word = row[1]\n        # Convert the Latin word to a tensor of integer indices\n        latin_indices = word_to_indices(latin_word, max_length_latin,char_to_idx_latin)\n        devanagari_indices= word_to_indices(devanagari_word,max_length_devanagari ,char_to_idx_lang)\n        pairs_t.append([latin_indices,devanagari_indices])","metadata":{"id":"OQlR-LjU-oUJ","execution":{"iopub.status.busy":"2024-05-06T08:44:09.306486Z","iopub.execute_input":"2024-05-06T08:44:09.306843Z","iopub.status.idle":"2024-05-06T08:44:14.609444Z","shell.execute_reply.started":"2024-05-06T08:44:09.306815Z","shell.execute_reply":"2024-05-06T08:44:14.608655Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Loading Data","metadata":{"id":"akPkrBg3QrqC"}},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(pairs, batch_size=64, shuffle=True)\nval_dataloader = torch.utils.data.DataLoader(pairs_v, batch_size=64, shuffle=False)\ntest_dataloader = torch.utils.data.DataLoader(pairs_t, batch_size=64, shuffle=False)","metadata":{"id":"_35IkD1e-oUM","execution":{"iopub.status.busy":"2024-05-06T08:44:14.613044Z","iopub.execute_input":"2024-05-06T08:44:14.613413Z","iopub.status.idle":"2024-05-06T08:44:14.619078Z","shell.execute_reply.started":"2024-05-06T08:44:14.613387Z","shell.execute_reply":"2024-05-06T08:44:14.618127Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Sweep Configuration","metadata":{"id":"g536wVKNQ1XU"}},{"cell_type":"code","source":"wandb.login(key=\"494428cc53b5c21da594f4fc75035d136c63a93c\")\npName = \"CS6910_Assignment3\"\nrun_obj=wandb.init( project=pName)\n\n# # Set up the configuration for the sweep using the `wandb.sweep` function\n# sweep_config = {\n#     'method': 'bayes',\n#     'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n#     'parameters': {\n        \n#         'drop_out': {\"values\": [0,0.2, 0.3]},\n\n#         'input_embedding_size': {\"values\": [16,32,64,256]},\n\n#         'hidden_layer_size': {\"values\": [16,32,64,256]},\n\n\n#         'number_of_encoder_layers': {\"values\": [1, 2, 3]},\n\n#         'number_of_decoder_layers': {\"values\": [1, 2, 3]},\n\n\n#         \"cell_type\": {\n#               \"values\": [ \"RNN\", \"GRU\", \"LSTM\"]\n#           },\n          \n#           \"learning_rate\": {\n#               \"values\": [1e-3, 1e-4]\n#           },\n       \n#         \"bidirectional\":{\n#             \"values\":[True, False]\n#         },\n\n#           \"epochs\": {\n#               \"values\": [10, 15, 20]\n#           },\n#           \"attention\": {\n#               \"values\": [True, False]\n#           },\n          \n#     }\n# }","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"KZ1QyWsV-oUN","outputId":"0fabf021-ae04-485c-b227-4938cbd538b7","execution":{"iopub.status.busy":"2024-05-06T08:44:14.620196Z","iopub.execute_input":"2024-05-06T08:44:14.620562Z","iopub.status.idle":"2024-05-06T08:44:33.428947Z","shell.execute_reply.started":"2024-05-06T08:44:14.620525Z","shell.execute_reply":"2024-05-06T08:44:33.427545Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m032\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240506_084416-cj3xvw0e</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m032/CS6910_Assignment3/runs/cj3xvw0e' target=\"_blank\">earthy-dream-14</a></strong> to <a href='https://wandb.ai/cs23m032/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m032/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs23m032/CS6910_Assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m032/CS6910_Assignment3/runs/cj3xvw0e' target=\"_blank\">https://wandb.ai/cs23m032/CS6910_Assignment3/runs/cj3xvw0e</a>"},"metadata":{}}]},{"cell_type":"markdown","source":"Model Without Attention","metadata":{"id":"xx7qY6JX-oUP"}},{"cell_type":"code","source":"import pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, embedding_size,cell_type,drop_out,num_layers,bidirectional):\n        '''Initialize the Encoder class.\n              \n        Args:\n            input_dim (int): Dimensionality of the input sequence.\n            hidden_dim (int): Dimensionality of the hidden state in the encoder.\n            embedding_size (int): Size of the input embedding.\n            cell_type (str): Type of the RNN cell to use (e.g., LSTM, GRU).\n            drop_out (float): Dropout probability to apply to the RNN output.\n            num_layers (int): Number of layers in the encoder.\n            bidirectional (bool): Flag indicating if the encoder is bidirectional.\n\n        '''\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.bidirectional = bidirectional\n        self.embedding = nn.Embedding(input_dim, embedding_size)\n\n        if(cell_type==\"GRU\"):\n          self.rnn = nn.GRU(embedding_size, hidden_dim,dropout=drop_out,num_layers=num_layers,bidirectional=bidirectional)\n        if(cell_type==\"LSTM\"):\n          self.rnn = nn.LSTM(embedding_size, hidden_dim,dropout=drop_out,num_layers=num_layers,bidirectional=bidirectional)\n        if(cell_type==\"RNN\"): \n          self.rnn = nn.RNN(embedding_size, hidden_dim,dropout=drop_out,num_layers=num_layers,bidirectional=bidirectional)\n\n    def forward(self, x):\n        '''Perform forward pass in the Encoder.\n        \n        Encode the input sequence into a fixed-size representation.\n        \n        Args:\n            x (Tensor): Input tensor of shape (sequence_length, batch_size).\n\n        Returns:\n            Tensor: Encoded representation of the input sequence.\n\n        '''\n        embedded = self.embedding(x)\n        output, hidden = self.rnn(embedded)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, hidden_dim,embedding_size ,cell_type,drop_out,num_layers,bidirectional):\n        '''Initialize the Decoder class.\n                \n        Args:\n            output_dim (int): Dimensionality of the output sequence.\n            hidden_dim (int): Dimensionality of the hidden state in the decoder.\n            embedding_size (int): Size of the input embedding.\n            cell_type (str): Type of the RNN cell to use (e.g., LSTM, GRU).\n            drop_out (float): Dropout probability to apply to the RNN output.\n            num_layers (int): Number of layers in the decoder.\n            bidirectional (bool): Flag indicating if the decoder is bidirectional.\n\n        '''\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Embedding(output_dim, embedding_size)\n        self.bidirectional = bidirectional\n        self.cell_type=cell_type\n        D=1\n        if self.bidirectional:\n          D=2\n\n        if(cell_type==\"GRU\"):\n          self.rnn = nn.GRU(embedding_size, hidden_dim,dropout=drop_out,num_layers=num_layers ,bidirectional=bidirectional)\n        elif(cell_type==\"LSTM\"):\n          self.rnn = nn.LSTM(embedding_size, hidden_dim,dropout=drop_out,num_layers=num_layers,bidirectional=bidirectional)\n        elif(cell_type==\"RNN\"):\n          self.rnn = nn.RNN(embedding_size, hidden_dim,dropout=drop_out,num_layers=num_layers,bidirectional=bidirectional)\n\n        self.fc = nn.Linear(hidden_dim*D, output_dim)\n\n    def forward(self, x, hidden):\n        '''Perform forward pass in the Decoder.\n        \n        Generate the output sequence by decoding the encoded representation.\n        \n        Args:\n            x (Tensor): Input tensor of shape (sequence_length, batch_size).\n            hidden (Tensor): Initial hidden state of the decoder.\n\n        Returns:\n            Tensor: Output tensor of shape (sequence_length, batch_size, output_dim).\n            Tensor: Updated hidden state of the decoder.\n\n        '''\n        x = x.unsqueeze(0)\n        embedded = self.embedding(x)\n        output, hidden = self.rnn(embedded, hidden)\n        prediction = self.fc(output.squeeze(0))\n        return prediction, hidden\n\nimport torch.nn.functional as F\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Seq2Seq(pl.LightningModule):\n    def __init__(self, input_dim, output_dim, hidden_dim,embedding_size, cell_type, drop_out,num_layers_encoders,num_layers_decoders,bidirectional,learning_rate):\n        '''Initialize the Seq2Seq model.\n        \n        Define the architecture and parameters of the sequence-to-sequence model.\n        \n        Args:\n            input_dim (int): Dimensionality of the input sequence.\n            output_dim (int): Dimensionality of the output sequence.\n            hidden_dim (int): Dimensionality of the hidden state in the encoder and decoder.\n            embedding_size (int): Size of the input embedding.\n            cell_type (str): Type of the RNN cell to use (e.g., LSTM, GRU).\n            drop_out (float): Dropout probability to apply to the RNN output.\n            num_layers_encoders (int): Number of layers in the encoder.\n            num_layers_decoders (int): Number of layers in the decoder.\n            bidirectional (bool): Flag indicating if the encoder and decoder are bidirectional.\n            learning_rate (float): Learning rate for the optimizer.\n\n        '''\n        super().__init__()\n\n        self.val_loss=[]\n        self.test_loss=[]\n        self.train_loss=[]\n\n        self.train_accuracy=[]\n        self.val_accuracy=[]\n        self.test_accuracy=[]\n\n        self.encoder = Encoder(input_dim, hidden_dim, embedding_size,cell_type,drop_out,num_layers_encoders,bidirectional)\n        self.decoder = Decoder(output_dim, hidden_dim, embedding_size, cell_type,drop_out,num_layers_decoders,bidirectional)\n        self.num_layers_encoders=num_layers_encoders\n        self.num_layers_decoders=num_layers_decoders\n\n        self.learning_rate=learning_rate\n        self.cell_type=cell_type\n\n        self.bidirectional = bidirectional\n        self.D=1\n        if self.bidirectional:\n          self.D=2\n\n    def forward(self, src, trg,teacher_forcing_ratio=0.5):\n        '''Perform forward pass in the Seq2Seq model.\n\n        Encode the source sequence, then decode it to generate the target sequence.\n        \n        Args:\n            src (Tensor): Source input tensor of shape (source_sequence_length, batch_size).\n            trg (Tensor): Target input tensor of shape (target_sequence_length, batch_size).\n            teacher_forcing_ratio (float): Probability of using teacher forcing during decoding.\n\n        Returns:\n            Tensor: Output tensor of shape (target_sequence_length, batch_size, output_dim).\n\n        '''\n        batch_size = trg.shape[0]\n        max_len = trg.shape[1]\n        trg_vocab_size = self.decoder.fc.out_features\n        src = src.transpose(0,1)\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n        hidden = self.encoder(src)\n\n        if(self.num_layers_encoders>self.num_layers_decoders):\n          if(self.cell_type==\"LSTM\"):\n            (hidden,cell)=hidden\n            difference=self.num_layers_encoders*self.D-self.num_layers_decoders*self.D\n            hidden=hidden[difference:]\n            cell=cell[difference:]\n            #from the difference to the last\n            hidden=(hidden,cell)\n\n          else:  \n            difference=self.num_layers_encoders*self.D-self.num_layers_decoders*self.D\n            hidden=hidden[difference:]  #from the difference to the last\n\n        elif(self.num_layers_encoders<self.num_layers_decoders):\n          cell=[]\n          if(self.cell_type==\"LSTM\"):\n            (hidden,cell)=hidden\n            \n            last_hidden = hidden[-self.D:]  # Shape: [1, 64, 256]\n            last_cell = cell[-self.D:]\n            for i in range(self.num_layers_encoders,self.num_layers_decoders):\n              hidden = torch.cat([hidden, last_hidden], dim=0)  # Shape: [4, 64, 256]\n              cell = torch.cat([cell, last_cell], dim=0)  # Shape: [4, 64, 256]\n            hidden=(hidden,cell)\n\n          else:\n\n            last_hidden = hidden[-self.D:] # Shape: [1, 64, 256]\n\n            for i in range(self.num_layers_encoders,self.num_layers_decoders):\n              hidden = torch.cat([hidden, last_hidden], dim=0)  # Shape: [4, 64, 256]\n\n        input = trg[:,0]#start character\n        for t in range(1, max_len):\n            output, hidden = self.decoder(input, hidden)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            input = top1 if teacher_forcing_ratio < torch.rand(1).item() else trg[:,t]\n            #if teacher_forcing_ratio > the random number generated then use the predicted character at the timestep t in timestamp t+1, else use the true value from the target \n        return outputs\n\n    def training_step(self, batch, batch_idx):\n        '''Perform a single training step.\n        \n        Compute the loss, update the model's parameters, and return the loss value.\n        \n        Args:\n            batch (tuple): Batch of input and target sequences.\n            batch_idx (int): Index of the current batch.\n\n        Returns:\n            Tensor: Loss value.\n\n        '''\n\n        src, trg = batch\n        # Forward pass through the model\n        output = self(src, trg)\n        outputAcc = self(src, trg)\n        trgAcc=trg \n\n        outputAcc = outputAcc.permute(1, 0, 2)\n        cols = torch.arange(output.shape[1]).unsqueeze(1)        \n        rows = torch.arange(output.shape[0])\n        # Create a tensor of zeros for expected values and assign 1 at corresponding positions\n        expected = torch.zeros(size=output.shape)\n        expected[rows, cols, trg.cpu()] = 1\n        # Get the output and expected values in the appropriate shape for loss calculation\n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        expected = expected[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n        train_loss = self.loss_fn(output.to(device), expected.to(device))\n\n        train_accuracy =self.accuracy(outputAcc, trgAcc)    #trg is the true value\n        self.train_accuracy.append(torch.tensor(train_accuracy))\n        self.train_loss.append(torch.tensor(train_loss))\n\n        return {'loss': train_loss}\n\n    def validation_step(self, batch, batch_idx):\n        '''Perform a single validation step.\n        \n        Evaluate the model's performance on a validation batch and return the results.\n        \n        Args:\n            batch (tuple): Batch of input and target sequences.\n            batch_idx (int): Index of the current batch.\n\n        '''\n\n\n        src, trg = batch\n        # Forward pass through the model with a flag value of 0\n\n        output = self(src, trg,0)\n        outputAcc = self(src, trg,0)\n        trgAcc=trg\n        # Permute the dimensions of outputAcc for further processing\n        outputAcc = outputAcc.permute(1, 0, 2)\n\n        cols = torch.arange(output.shape[1]).unsqueeze(1)\n        rows = torch.arange(output.shape[0])\n        # Create a tensor of zeros for expected values and assign 1 at corresponding positions\n        expected = torch.zeros(size=output.shape)\n        expected[rows, cols, trg.cpu()] = 1\n        # Get the output and expected values in the appropriate shape for loss calculation\n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        expected = expected[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n\n        val_loss = self.loss_fn(output.to(device), expected.to(device))\n\n        val_accuracy =self.accuracy(outputAcc, trgAcc)   \n        self.val_accuracy.append(torch.tensor(val_accuracy))\n        self.val_loss.append(torch.tensor(val_loss))\n        return {'loss': val_loss}\n\n    def test_step(self, batch, batch_idx):\n        '''Perform a single test step.\n        \n        Evaluate the model's performance on a test batch and return the results.\n        \n        Args:\n            batch (tuple): Batch of input and target sequences.\n            batch_idx (int): Index of the current batch.\n\n        '''\n        src, trg = batch\n        # Get the output and expected values for evaluation\n        output = self(src, trg,0)\n        outputAcc = self(src, trg,0)\n        trgAcc=trg\n\n        outputAcc = outputAcc.permute(1, 0, 2)\n        # Create indices for indexing the expected tensor\n        cols = torch.arange(output.shape[1]).unsqueeze(1)\n        rows = torch.arange(output.shape[0])\n        # Create a tensor of zeros with the same size as output for expected values\n        expected = torch.zeros(size=output.shape)\n        expected[rows, cols, trg.cpu()] = 1\n        # Reshape the output and expected tensors for loss calculation\n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        expected = expected[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n\n        test_loss = self.loss_fn(output.to(device), expected.to(device))\n        test_accuracy =self.accuracy(outputAcc, trgAcc)    #trg is the true value\n        grid_input,grid_target,grid_predicted=self.grid(src,outputAcc, trgAcc)\n\n        # Convert grid target and predicted values to string representations\n        target_outputs=[]\n        str_target=\"\"\n        for i in grid_target:\n          for j in i:\n            integer_value = j.item()\n            str_target=str_target+get_key(j)\n          target_outputs.append(str_target)\n          str_target=\"\"\n\n        predicted_outputs=[]\n        str_predicted=\"\"\n        for i in grid_predicted:\n          for j in i:\n            integer_value = j.item()\n            str_predicted=str_predicted+get_key(j)\n          predicted_outputs.append(str_predicted)\n          str_predicted=\"\"\n\n        inputs=[]\n        str_input=\"\"\n        for i in grid_input:\n          for j in i:\n            integer_value = j.item()\n            str_input=str_input+get_key_input(j)\n          inputs.append(str_input)\n          str_input=\"\"\n\n        # Append test accuracy and loss to their respective lists\n        self.test_accuracy.append(torch.tensor(test_accuracy))\n        self.test_loss.append(torch.tensor(test_loss))\n#         print({\"test_loss\":test_loss,\"test_accuracy\":test_accuracy})\n        wandb.log({\"test_loss\":test_loss,\"test_accuracy\":test_accuracy})\n\n        save_outputs_to_csv(inputs,target_outputs, predicted_outputs)\n        return {'loss':test_loss}\n\n\n    def configure_optimizers(self):\n        \"\"\"\n        Configure the optimizer.\n        \"\"\"\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate )\n        return optimizer\n\n    def loss_fn(self, output, trg):\n        \"\"\"\n        Calculate the loss function.\n        \"\"\"\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(output, trg)\n        return loss.mean()\n\n    def accuracy(self, output, target):\n      \"\"\"\n      Calculate the accuracy.\n      \"\"\"\n      predicted = output.argmax(dim=-1)  # shape: (sequence_length, batch_size)\n      equal_rows = 0\n      for i in range(target.size(0)):\n          # Remove the first and last indices and check if the sliced tensors are equal\n          if torch.all(torch.eq(target[i, 1:-1], predicted[i, 1:-1])):\n              equal_rows += 1\n      matches=equal_rows\n\n      # Compute the accuracy\n      accuracy = matches / len(target) * 100\n      return accuracy\n\n    # def grid(self, output, target):\n    #   \"\"\"\n    #   Generate grids for target and expected outputs.\n    #   \"\"\"\n    #   grid_target=[]\n    #   grid_expected=[]\n    #   predicted = output.argmax(dim=-1)  # shape: (sequence_length, batch_size)\n    #   for i in range(target.size(0)):\n    #       # Remove the first and last indices and check if the sliced tensors are equal\n    #       grid_target.append(target[i, 1:-1])\n    #       grid_expected.append(predicted[i, 1:-1])\n\n    #   return grid_target,grid_expected\n\n    def grid(self,input, output, target):\n      \"\"\"\n      Generate grids for target and expected outputs.\n      \"\"\"\n      grid_input=[]\n      grid_target=[]\n      grid_expected=[]\n      predicted = output.argmax(dim=-1) \n      for i in range(target.size(0)):\n          # Remove the first and last indices and check if the sliced tensors are equal\n          grid_target.append(target[i, 1:-1])\n          grid_expected.append(predicted[i, 1:-1])\n          grid_input.append(input[i, 1:-1])\n      return grid_input,grid_target,grid_expected\n\n\n    def on_train_epoch_end(self):\n      \"\"\"\n      Actions to perform at the end of each training epoch.\n      \"\"\"\n      train_loss=torch.stack(self.train_loss).mean()\n      self.train_loss=[]\n\n      val_loss=torch.stack(self.val_loss).mean()\n      self.val_loss=[]\n\n      train_accuracy=torch.stack(self.train_accuracy).mean()\n      self.train_accuracy=[]\n\n      val_accuracy=torch.stack(self.val_accuracy).mean()\n      self.val_accuracy=[]\n#       print({\"train_loss\":train_loss,\"train_accuracy\":train_accuracy,\"val_loss\":val_loss,\"val_accuracy\":val_accuracy})\n      wandb.log({\"train_loss\":train_loss,\"train_accuracy\":train_accuracy,\"val_loss\":val_loss,\"val_accuracy\":val_accuracy})\n\n# function to return key for any value \ndef get_key(val):\n    for key, value in char_to_idx_lang.items():\n        if val == value:\n            return key\n    return \"\"\n\ndef get_key_input(val):\n    for key, value in char_to_idx_latin.items():\n        if val == value:\n            return key\n    return \"\"\n\ndef beam_search(self, encoder_output, encoder_hidden, decoder_input, beam_width,batch_size,max_len,trg_vocab_size,output):\n    # Set initial beam\n    beam = [(decoder_input, 0, encoder_hidden)]\n    outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n\n\n    # Iterate through the sequence\n    for i in range(self.max_seq_len):\n        new_beam = []\n        for j in range(len(beam)):\n            decoder_input = beam[j][0]\n            decoder_hidden = beam[j][2]\n\n            # Run the decoder\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output)\n\n            # Get the top-k most probable tokens\n            topk_probs, topk_indices = torch.topk(decoder_output, beam_width)\n\n            for k in range(beam_width):\n                # Calculate the new score\n                score = beam[j][1] + topk_probs[0][k]\n\n                # Create a new hypothesis\n                hypothesis = (\n                    torch.tensor([topk_indices[0][k]]).to(self.device),\n                    score,\n                    decoder_hidden\n                )\n\n                # Add it to the new beam\n                new_beam.append(hypothesis)\n        outputs[t] = output\n\n        # Select the top-k hypotheses\n        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n\n    return \n\n# importing pandas as pd  \nimport pandas as pd  \nimport os\n\ndef save_outputs_to_csv(inputs,target_outputs, predicted_outputs):\n    file_exists = os.path.exists('Output.csv')\n    # dictionary of lists  \n    dict = {'input':inputs,'target':target_outputs, 'predicted': predicted_outputs}  \n    df = pd.DataFrame(dict) \n    # saving the dataframe \n    df.to_csv('Output.csv',mode='a',index=False,header=not file_exists) ","metadata":{"id":"hyflCGQh-oUU","execution":{"iopub.status.busy":"2024-05-06T08:44:33.431222Z","iopub.execute_input":"2024-05-06T08:44:33.431533Z","iopub.status.idle":"2024-05-06T08:44:37.499314Z","shell.execute_reply.started":"2024-05-06T08:44:33.431474Z","shell.execute_reply":"2024-05-06T08:44:37.498330Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Model With Attention\n","metadata":{"id":"_cMJrYHe-oUe"}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass AttnEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, embedding_size,cell_type,drop_out,num_layers,bidirectional):\n\n        \"\"\"\n        Initializes the Attention Encoder module.\n\n        Args:\n            input_dim (int): Dimensionality of the input.\n            hidden_dim (int): Dimensionality of the hidden state/output.\n            embedding_size (int): Size of the embedding for input sequence.\n            cell_type (str): Type of RNN cell to use (GRU, LSTM, RNN).\n            drop_out (float): Dropout rate.\n            num_layers (int): Number of layers in the RNN.\n            bidirectional (bool): Flag indicating whether to use bidirectional RNN.\n\n        \"\"\"\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.bidirectional = bidirectional\n        self.embedding = nn.Embedding(input_dim, embedding_size)\n\n        if(cell_type==\"GRU\"):\n          self.rnn = nn.GRU(embedding_size, hidden_dim,dropout=drop_out,num_layers=num_layers,bidirectional=bidirectional)\n        if(cell_type==\"LSTM\"):\n          self.rnn = nn.LSTM(embedding_size, hidden_dim,dropout=drop_out,num_layers=num_layers,bidirectional=bidirectional)\n        if(cell_type==\"RNN\"): \n          self.rnn = nn.RNN(embedding_size, hidden_dim,dropout=drop_out,num_layers=num_layers,bidirectional=bidirectional)\n\n    def forward(self, x):\n        \"\"\"\n        Performs a forward pass of the Attention Encoder module.\n\n        Args:\n            x (tensor): Input tensor of shape (seq_len, batch_size).\n\n        Returns:\n            output (tensor): Output tensor from the RNN layer.\n            hidden (tensor): Hidden state from the RNN layer.\n\n        \"\"\"\n        embedded = self.embedding(x)\n        output, hidden = self.rnn(embedded)\n        return output, hidden\n\nclass AttnDecoder(nn.Module):\n    def __init__(self, output_dim, hidden_dim,embedding_size ,cell_type,drop_out,num_layers,bidirectional,max_length):\n        \"\"\"\n        Initializes the Attention Decoder module.\n\n        Args:\n            output_dim (int): Dimensionality of the output.\n            hidden_dim (int): Dimensionality of the hidden state/output.\n            embedding_size (int): Size of the embedding for output sequence.\n            cell_type (str): Type of RNN cell to use (GRU, LSTM, RNN).\n            drop_out (float): Dropout rate.\n            num_layers (int): Number of layers in the RNN.\n            bidirectional (bool): Flag indicating whether to use bidirectional RNN.\n            max_length (int): Maximum length of the input sequence.\n\n        \"\"\"\n\n        self.max_length=max_length+2\n        super(AttnDecoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Embedding(output_dim, embedding_size)\n        self.attn = nn.Linear(self.hidden_dim + embedding_size, self.max_length)\n        self.bidirectional = bidirectional\n        self.cell_type=cell_type\n        D=1\n        if self.bidirectional:\n          D=2\n        self.attn_combine = nn.Linear(self.hidden_dim*D + embedding_size, self.hidden_dim)\n\n        if(cell_type==\"GRU\"):\n          self.rnn = nn.GRU(hidden_dim, hidden_dim,dropout=drop_out,num_layers=num_layers ,bidirectional=bidirectional)\n        elif(cell_type==\"LSTM\"):\n          self.rnn = nn.LSTM(hidden_dim, hidden_dim,dropout=drop_out,num_layers=num_layers,bidirectional=bidirectional)\n        elif(cell_type==\"RNN\"):\n          self.rnn = nn.RNN(hidden_dim, hidden_dim,dropout=drop_out,num_layers=num_layers,bidirectional=bidirectional)\n\n        self.fc = nn.Linear(hidden_dim*D, output_dim)\n\n    def forward(self, x, hidden, encoder_outputs):\n        \"\"\"\n        Performs a forward pass of the Attention Decoder module.\n\n        Args:\n            x (tensor): Input tensor of shape (batch_size).\n            hidden (tensor): Hidden state from the previous time step.\n            encoder_outputs (tensor): Encoder outputs of shape (seq_len, batch_size, hidden_dim).\n\n        Returns:\n            prediction (tensor): Output tensor from the linear layer.\n            hidden (tensor): Hidden state for the current time step.\n            attn_weights (tensor): Attention weights for the current time step.\n\n        \"\"\"\n        x = x.unsqueeze(1).transpose(0,1)\n        embedded = self.embedding(x)\n        # Calculate attention weights based on the cell type\n        if(self.cell_type==\"LSTM\"):\n          attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1)\n        else:\n          attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n        # Calculate the attention applied to encoder outputs\n        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n                                 encoder_outputs.permute(1,0,2))\n        # Concatenate embedded input and attention applied tensor\n        output = torch.cat((embedded[0], attn_applied.squeeze(1)), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n        # Adjust hidden and cell variables based on the cell type\n        if(self.cell_type==\"LSTM\"):\n          (hidden,cell)=hidden\n\n        if(self.cell_type==\"LSTM\"):\n          output, hidden = self.rnn(output, (hidden,cell))\n        else:\n          output, hidden = self.rnn(output, hidden)\n\n        prediction = self.fc(output.squeeze(0))\n        # Return prediction, hidden state, and attention weights\n        return prediction, hidden, attn_weights\n\nimport torch.nn.functional as F\n\nclass Seq2SeqAttn(pl.LightningModule):\n    def __init__(self, input_dim, output_dim, hidden_dim,embedding_size, cell_type, drop_out,num_layers_encoders,num_layers_decoders,bidirectional,learning_rate,max_length_latin):\n        \"\"\"\n        Initializes the Seq2Seq model.\n\n        Args:\n            input_dim (int): Dimensionality of the input sequence.\n            output_dim (int): Dimensionality of the output sequence.\n            hidden_dim (int): Dimensionality of the hidden state/output.\n            embedding_size (int): Size of the embedding for input/output sequences.\n            cell_type (str): Type of RNN cell to use (GRU, LSTM, RNN).\n            drop_out (float): Dropout rate.\n            num_layers (int): Number of layers in the RNN.\n            bidirectional (bool): Flag indicating whether to use bidirectional RNN.\n            max_length (int): Maximum length of the input sequence.\n\n        \"\"\"\n        super().__init__()\n\n        self.val_loss=[]\n        self.train_loss=[]\n\n        self.train_accuracy=[]\n        self.val_accuracy=[]\n\n        self.test_accuracy=[]\n        self.test_loss=[]\n\n        self.encoder = AttnEncoder(input_dim, hidden_dim, embedding_size,cell_type,drop_out,num_layers_encoders,bidirectional)\n        self.decoder = AttnDecoder(output_dim, hidden_dim, embedding_size, cell_type,drop_out,num_layers_decoders,bidirectional,max_length_latin)\n\n        self.num_layers_encoders=num_layers_encoders\n        self.num_layers_decoders=num_layers_decoders\n\n        self.learning_rate=learning_rate\n        self.cell_type=cell_type\n\n        self.bidirectional = bidirectional\n        self.D=1\n        if self.bidirectional:\n          self.D=2\n        self.attn_weights=[]\n        self.counter=0\n\n\n    def forward(self, src, trg,teacher_forcing_ratio=0.5):\n        \"\"\"\n        Performs a forward pass of the Seq2Seq model.\n\n        Args:\n            source (tensor): Input tensor of shape (seq_len, batch_size).\n            target (tensor): Target tensor of shape (seq_len, batch_size).\n            teacher_forcing_ratio (float): The probability of using teacher forcing.\n\n        Returns:\n            outputs (tensor): Output tensor from the decoder of shape (seq_len, batch_size, output_dim).\n\n        \"\"\"\n        batch_size = trg.shape[0]\n        max_len = trg.shape[1]\n        trg_vocab_size = self.decoder.fc.out_features\n        # Transpose the source tensor\n        src = src.transpose(0,1)\n        # Initialize outputs tensor with zeros\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n        attentionV = torch.zeros(max_len, batch_size, max_length_latin+2).to(self.device)\n\n        encoder_output,hidden = self.encoder(src)\n\n        input = trg[:,0] #start character\n        # Iterate over each timestep in the target sequence\n        for t in range(1, max_len):\n            # Pass the input, hidden state, and encoder output to the decoder\n\n            output, hidden,attentionV[t] = self.decoder(input, hidden,encoder_output)\n            outputs[t] = output\n            # Determine the next input based on teacher forcing ratio\n            top1 = output.argmax(1)\n            \n            input = top1 if teacher_forcing_ratio < torch.rand(1).item() else trg[:,t]\n            #if teacher_forcing_ratio > the random number generated then use the predicted character at the timestep t in timestamp t+1, else use the true value from the target \n\n        return outputs, attentionV\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Trains the Seq2Seq model.\n\n        Args:\n            model (nn.Module): The Seq2Seq model to train.\n            iterator (DataLoader): The data iterator.\n            optimizer (torch.optim.Optimizer): The optimizer to use for training.\n            criterion (nn.Module): The loss function.\n            clip (float): The value to clip the gradients.\n\n        Returns:\n            epoch_loss (float): The average loss for the epoch.\n\n        \"\"\"\n        # Get the source and target tensors from the batch\n        src, trg = batch\n        output, attentionV = self(src, trg)\n        outputAcc, attentionVD = self(src, trg)\n        trgAcc=trg \n        # Permute the output for accuracy calculation\n        outputAcc = outputAcc.permute(1, 0, 2)\n        cols = torch.arange(output.shape[1]).unsqueeze(1)        \n        rows = torch.arange(output.shape[0])\n        # Create a tensor for expected values and assign 1 to the corresponding indices\n        expected = torch.zeros(size=output.shape)\n        expected[rows, cols, trg.cpu()] = 1\n\n        output_dim = output.shape[-1]\n        # Remove the start token from the output and reshape for loss calculation\n        output = output[1:].view(-1, output_dim)\n        expected = expected[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n        train_loss = self.loss_fn(output.to(device), expected.to(device))\n        train_accuracy =self.accuracy(outputAcc, trgAcc)    #trg is the true value\n        # Append the training accuracy and loss to their respective lists\n        self.train_accuracy.append(torch.tensor(train_accuracy))\n        self.train_loss.append(torch.tensor(train_loss))\n        # del src\n        # del trg\n        return {'loss': train_loss}\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        Perform a validation step.\n        \"\"\"\n        src, trg = batch\n        output,attentionV = self(src, trg,0)\n        outputAcc,attentionV = self(src, trg,0)\n        trgAcc=trg\n        # Permute the output for accuracy calculation\n        outputAcc = outputAcc.permute(1, 0, 2)\n        # Create indices for expected values\n        cols = torch.arange(output.shape[1]).unsqueeze(1)\n        rows = torch.arange(output.shape[0])\n        expected = torch.zeros(size=output.shape)\n        expected[rows, cols, trg.cpu()] = 1\n        output_dim = output.shape[-1]\n        # Remove the start token from the output and reshape for loss calculation\n        output = output[1:].view(-1, output_dim)\n        expected = expected[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n\n        val_loss = self.loss_fn(output.to(device), expected.to(device))\n        val_accuracy =self.accuracy(outputAcc, trgAcc)    #trg is the true value\n        # Append the training accuracy and loss to their respective lists\n        self.val_accuracy.append(torch.tensor(val_accuracy))\n        self.val_loss.append(torch.tensor(val_loss))\n        return {'loss': val_loss}\n\n\n    def configure_optimizers(self):\n        \"\"\"\n        Configure the optimizer.\n        \"\"\"\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate )\n        return optimizer\n\n    def loss_fn(self, output, trg):\n        \"\"\"\n        Calculate the loss function.\n        \"\"\"\n        criterion = nn.CrossEntropyLoss()\n        loss = criterion(output, trg)\n        return loss.mean()\n\n    def accuracy(self, output, target):\n      \"\"\"\n      Calculate the loss function.\n      \"\"\"\n      predicted = output.argmax(dim=-1)  # shape: (sequence_length, batch_size)\n      equal_rows = 0\n      for i in range(target.size(0)):\n          # Remove the first and last indices and check if the sliced tensors are equal\n          if torch.all(torch.eq(target[i, 1:-1], predicted[i, 1:-1])):\n              equal_rows += 1\n      matches=equal_rows\n\n      # Compute the accuracy\n      accuracy = matches / len(target) * 100\n      return accuracy\n\n    def grid(self,input, output, target):\n      \"\"\"\n      Generate grids for target and expected outputs.\n      \"\"\"\n      grid_input=[]\n      grid_target=[]\n      grid_expected=[]\n      predicted = output.argmax(dim=-1) \n      for i in range(target.size(0)):\n          # Remove the first and last indices and check if the sliced tensors are equal\n          grid_target.append(target[i, 1:-1])\n          grid_expected.append(predicted[i, 1:-1])\n          grid_input.append(input[i, 1:-1])\n      return grid_input,grid_target,grid_expected\n\n\n    def on_train_epoch_end(self):\n      \"\"\"\n      Actions to perform at the end of each training epoch.\n      \"\"\"\n      train_loss=torch.stack(self.train_loss).mean()\n      self.train_loss=[]\n\n      val_loss=torch.stack(self.val_loss).mean()\n      self.val_loss=[]\n\n      train_accuracy=torch.stack(self.train_accuracy).mean()\n      self.train_accuracy=[]\n\n      val_accuracy=torch.stack(self.val_accuracy).mean()\n      self.val_accuracy=[]\n#       print({\"train_loss\":train_loss,\"train_accuracy\":train_accuracy,\"val_loss\":val_loss,\"val_accuracy\":val_accuracy})\n      wandb.log({\"train_loss\":train_loss,\"train_accuracy\":train_accuracy,\"val_loss\":val_loss,\"val_accuracy\":val_accuracy})\n\n\n    def test_step(self, batch, batch_idx):\n        \"\"\"\n        Perform a test step.\n        \"\"\"\n        src, trg = batch\n        output, attentionV = self(src, trg,0)\n        outputAcc, attentionVD = self(src, trg,0)\n        trgAcc=trg\n        # Permute the output for accuracy calculation\n        outputAcc = outputAcc.permute(1, 0, 2)\n        cols = torch.arange(output.shape[1]).unsqueeze(1)\n        rows = torch.arange(output.shape[0])\n        # Create a tensor for expected values and assign 1 to the corresponding indices\n        expected = torch.zeros(size=output.shape)\n        expected[rows, cols, trg.cpu()] = 1\n        output_dim = output.shape[-1]\n        # Remove the start token from the output and reshape for loss calculation\n        output = output[1:].view(-1, output_dim)\n        expected = expected[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n\n        test_loss = self.loss_fn(output.to(device), expected.to(device))\n        test_accuracy =self.accuracy(outputAcc, trgAcc)    #trg is the true value\n        grid_input,grid_target,grid_predicted=self.grid(src,outputAcc, trgAcc)\n\n        # Convert grid-based outputs to string representations\n        target_outputs=[]\n        str_target=\"\"\n        for i in grid_target:\n          for j in i:\n            integer_value = j.item()\n            str_target=str_target+get_key(j)\n          target_outputs.append(str_target)\n          str_target=\"\"\n        predicted_outputs=[]\n        str_predicted=\"\"\n        for i in grid_predicted:\n          for j in i:\n            integer_value = j.item()\n            str_predicted=str_predicted+get_keyAttn(j)\n          predicted_outputs.append(str_predicted)\n          str_predicted=\"\"\n\n        inputs=[]\n        str_input=\"\"\n        for i in grid_input:\n          for j in i:\n            integer_value = j.item()\n            str_input=str_input+get_key_input(j)\n          inputs.append(str_input)\n          str_input=\"\"\n\n        # Append test accuracy and loss to their respective lists\n        self.test_accuracy.append(torch.tensor(test_accuracy))\n        self.test_loss.append(torch.tensor(test_loss))\n#         print({\"test_loss\":test_loss,\"test_accuracy\":test_accuracy})\n        wandb.log({\"test_loss\":test_loss,\"test_accuracy\":test_accuracy})\n        # Save target and predicted outputs to a CSV file\n        save_outputs_to_csvAttn(inputs,target_outputs, predicted_outputs)\n        # plot_attention_weights(self.attn_weights)\n        if(self.counter<1):\n          s(inputs,predicted_outputs,attentionV)\n          self.counter=self.counter+1\n        return {'loss':test_loss}\n\n# function to return key for any value\ndef get_keyAttn(val):\n    for key, value in char_to_idx_lang.items():\n        if val == value:\n            return key\n    return \"\"\n    \ndef get_key_inputAttn(val):\n    for key, value in char_to_idx_latin.items():\n        if val == value:\n            return key\n    return \"\"\n\ndef beam_searchAttn(self, encoder_output, encoder_hidden, decoder_input, beam_width,batch_size,max_len,trg_vocab_size,output):\n    # Set initial beam\n    beam = [(decoder_input, 0, encoder_hidden)]\n    outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n\n    # Iterate through the sequence\n    for i in range(self.max_seq_len):\n        new_beam = []\n        for j in range(len(beam)):\n            decoder_input = beam[j][0]\n            decoder_hidden = beam[j][2]\n\n            # Run the decoder\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output)\n\n            # Get the top-k most probable tokens\n            topk_probs, topk_indices = torch.topk(decoder_output, beam_width)\n\n            for k in range(beam_width):\n                # Calculate the new score\n                score = beam[j][1] + topk_probs[0][k]\n\n                # Create a new hypothesis\n                hypothesis = (\n                    torch.tensor([topk_indices[0][k]]).to(self.device),\n                    score,\n                    decoder_hidden\n                )\n\n                # Add it to the new beam\n                new_beam.append(hypothesis)\n        outputs[t] = output\n\n        # Select the top-k hypotheses\n        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n    return \n\nimport pandas as pd  \n      \ndef save_outputs_to_csvAttn(inputs,target_outputs, predicted_outputs):\n    file_exists = os.path.exists('Output.csv')\n    # dictionary of lists  \n    dict = {'input':inputs,'target':target_outputs, 'predicted': predicted_outputs}  \n    df = pd.DataFrame(dict) \n    # saving the dataframe \n    df.to_csv('Output.csv',mode='a',index=False,header=not file_exists) \n\n\ndef s(input_words, output_words, attn_weights):\n    # Create a figure with a 3x3 grid and set the size to 10x10 inches\n    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n    \n    # Adjust the spacing between subplots\n    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n    \n    # Iterate over each index up to 9 and plot the attention weights\n    for i, ax in enumerate(axes.flat):\n        # Check if input and output words are provided\n        if i < len(input_words) and i < len(output_words):\n            # Get the attention weights for the corresponding input word\n            attn_weight = attn_weights[i].cpu().detach().numpy()\n            attn_weight = attn_weight[1:len(input_words[i]) + 1, :len(output_words[i])]\n            \n            # Plot the attention weights as a heatmap on the current axis\n            sns.heatmap(attn_weight, ax=ax, cmap='Blues', cbar=False)\n            \n            # Set the y-axis tick positions and labels to the input words and rotate them vertically\n            ax.set_yticks(range(len(input_words[i])))\n            ax.set_yticklabels(reversed(input_words[i]), rotation='vertical')\n            \n            # Set the x-axis tick positions and labels to the output words and rotate them horizontally\n            ax.set_xticks(range(len(output_words[i])))\n#             fontproperties=hindi_font,\n            ax.set_xticklabels(reversed(output_words[i]), rotation=45, ha='right')\n            \n            # Set the title of each subplot as the index number\n            ax.set_title(f'Attention {i+1}', fontsize=12)\n        else:\n            # If input or output words are missing, display a message in the subplot\n            ax.text(0.5, 0.5, 'Missing Data', horizontalalignment='center', verticalalignment='center', fontsize=12)\n            ax.axis('off')\n        \n    # Remove any unused subplots\n    for j in range(len(input_words), len(axes.flat)):\n        fig.delaxes(axes.flat[j])\n    \n    # Log the plot to Weights & Biases\n    wandb.log({\"Question 5\": wandb.Image(plt)})\n    \n    plt.show()\n","metadata":{"id":"6uWSEe9Y-oUf","execution":{"iopub.status.busy":"2024-05-06T08:44:37.500890Z","iopub.execute_input":"2024-05-06T08:44:37.501439Z","iopub.status.idle":"2024-05-06T08:44:37.592558Z","shell.execute_reply.started":"2024-05-06T08:44:37.501411Z","shell.execute_reply":"2024-05-06T08:44:37.591551Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8tsRCY4AK-wT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Running the wandb sweep","metadata":{"id":"vB4gwUdftBb6"}},{"cell_type":"code","source":"# def sweep():\n#     '''Perform hyperparameter sweep using WandB.\n    \n#     Initialize the experiment configuration, create the model, train the model,\n#     and log the results using WandB.\n\n#     '''\n#     wandb.init()\n#     config = wandb.config\n\n#     if(config.attention==False):\n#       model = Seq2Seq(len(char_to_idx_latin)+2, len(char_to_idx_lang)+2, config.hidden_layer_size, config.input_embedding_size, config.cell_type,config.drop_out,config.number_of_encoder_layers,config.number_of_decoder_layers,config.bidirectional,config.learning_rate)\n\n#     else:\n#       model = Seq2SeqAttn(len(char_to_idx_latin)+2, len(char_to_idx_lang)+2, config.hidden_layer_size, config.input_embedding_size, config.cell_type,config.drop_out,1,1,config.bidirectional,config.learning_rate, max_length_latin)\n\n#     model.to(device)\n#     # Create a Trainer for training the model\n#     trainer = pl.Trainer(max_epochs=config.epochs, accelerator=\"gpu\", devices=1)\n#     trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n\n#     # Generate a unique run name based on the hyperparameters\n#     run_name = \"lr_{}_hls_{}_es_{}_ct_{}_do_{}_bd_{}_lr_{}_att_{}\".format(config.learning_rate, config.hidden_layer_size,config.input_embedding_size,config.cell_type, config.drop_out,config.bidirectional,config.learning_rate,config.attention)\n#     print(run_name)\n#     wandb.run.name = run_name\n\n# # Initialize the WandB sweep\n# # sweep_id = wandb.sweep(sweep_config,project=pName)\n# # wandb.agent(sweep_id, sweep)","metadata":{"id":"4ETVcH-t-oUk","execution":{"iopub.status.busy":"2024-05-06T08:44:37.594005Z","iopub.execute_input":"2024-05-06T08:44:37.594376Z","iopub.status.idle":"2024-05-06T08:44:37.605157Z","shell.execute_reply.started":"2024-05-06T08:44:37.594341Z","shell.execute_reply":"2024-05-06T08:44:37.604188Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"##Training and Evaluating the model with best hyperparameters","metadata":{"id":"j91TCDDhVvng"}},{"cell_type":"code","source":"# hidden_layer_size=256\n# embedding_size= 256\n# cell_type=\"LSTM\"\n# drop_out=0.2\n# num_layers_encoders=3\n# num_layers_decoders=2\n# bidirectional=True\n# attention=False\n# learning_rate=0.001\n# epochs=15\n# if(attention==False):\n#   model = Seq2Seq(len(char_to_idx_latin)+2, len(char_to_idx_lang)+2, hidden_layer_size, embedding_size, cell_type,drop_out,num_layers_encoders,num_layers_decoders,bidirectional,learning_rate)\n\n# else:\n#   model = Seq2SeqAttn(len(char_to_idx_latin)+2, len(char_to_idx_lang)+2, hidden_layer_size, embedding_size, cell_type,drop_out,1,1,bidirectional,learning_rate, max_length_latin)\n\n# model.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0cHWLbf-oUl","outputId":"a79a8f14-f23b-40de-d696-dfe5abc7131c","execution":{"iopub.status.busy":"2024-05-06T08:44:37.606554Z","iopub.execute_input":"2024-05-06T08:44:37.607788Z","iopub.status.idle":"2024-05-06T08:44:37.613936Z","shell.execute_reply.started":"2024-05-06T08:44:37.607752Z","shell.execute_reply":"2024-05-06T08:44:37.612844Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# hidden_layer_size=256\n# embedding_size= 16\n# cell_type=\"LSTM\"\n# drop_out=0.2\n# num_layers_encoders=3\n# num_layers_decoders=3\n# bidirectional=True\n# attention=True\n# learning_rate=0.001\n# epochs=10\n# if(attention==False):\n#   model = Seq2Seq(len(char_to_idx_latin)+2, len(char_to_idx_lang)+2, hidden_layer_size, embedding_size, cell_type,drop_out,num_layers_encoders,num_layers_decoders,bidirectional,learning_rate)\n\n# else:\n#   model = Seq2SeqAttn(len(char_to_idx_latin)+2, len(char_to_idx_lang)+2, hidden_layer_size, embedding_size, cell_type,drop_out,1,1,bidirectional,learning_rate, max_length_latin)\n\n# model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:44:37.615062Z","iopub.execute_input":"2024-05-06T08:44:37.615448Z","iopub.status.idle":"2024-05-06T08:44:37.785864Z","shell.execute_reply.started":"2024-05-06T08:44:37.615425Z","shell.execute_reply":"2024-05-06T08:44:37.784832Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Seq2SeqAttn(\n  (encoder): AttnEncoder(\n    (embedding): Embedding(29, 16)\n    (rnn): LSTM(16, 256, dropout=0.2, bidirectional=True)\n  )\n  (decoder): AttnDecoder(\n    (embedding): Embedding(67, 16)\n    (attn): Linear(in_features=272, out_features=26, bias=True)\n    (attn_combine): Linear(in_features=528, out_features=256, bias=True)\n    (rnn): LSTM(256, 256, dropout=0.2, bidirectional=True)\n    (fc): Linear(in_features=512, out_features=67, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"hidden_layer_size=16\nembedding_size= 16\ncell_type=\"LSTM\"\ndrop_out=0.2\nnum_layers_encoders=2\nnum_layers_decoders=2\nbidirectional=True\nattention=True\nlearning_rate=0.001\nepochs=1\nif(attention==False):\n  model = Seq2Seq(len(char_to_idx_latin)+2, len(char_to_idx_lang)+2, hidden_layer_size, embedding_size, cell_type,drop_out,num_layers_encoders,num_layers_decoders,bidirectional,learning_rate)\n\nelse:\n  model = Seq2SeqAttn(len(char_to_idx_latin)+2, len(char_to_idx_lang)+2, hidden_layer_size, embedding_size, cell_type,drop_out,1,1,bidirectional,learning_rate, max_length_latin)\n\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init()\n# Create a Trainer for training the model\ntrainer = pl.Trainer(max_epochs=epochs, accelerator=\"gpu\", devices=1)\n# Set the run name in WandB\ntrainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n# Evaluate the trained model on the test dataset\ntrainer.test(model, test_dataloader)\nwandb.finish()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":891,"referenced_widgets":["e2c6e0ffa3594104a22359e77e3ab090","f9c6934d3cb244f7b20f46ca19a2c7c5","026718ca95cb4435802143337cd8b70b","d52f6ff85f5644e5bd73feebb3f1300f","7f9a0235399d4985a8363fc799a7ea24","3fc597a1bb7c4badb1db05e2674b4cc9","c7cb187525684de6b61ffb01c934626c","3bd0841df2e14a05b8a687578134272c","0499e51a615043cc9da4f5c47dedf592","c67069c5834d48ea8e58ae901c111615","0b33d8b3437046cbba0b33a75f0702be","5164305beb4945a7b6f67f60b28b161f","e11eb1abd3614bcba0776962fa536be5","3b47694ffe1740bfb4afc89b3aec8b83","75f1bae28b104aa3a6b8376ce6e67b16","76f860bac31848088056ad2f41efd906","90363bd470494db7b33a89ab8693cf26","5e98b49124f5456d8be4c101474aea87","e0b8f94992eb47e397d14f82d2e89e90","ede0515ca11f420f9d2c04593d256113","ef35712b7ef745a0b3e7d8cfb0d91bc3","43ca68c849b14af19f87274e8b9b00e0","d40b59a3ad5d42ae9c4dde8c415d1eb5","e90f9274ac9545c3ae5459fea17fbe32","230cc3dd6a394d90bf1c0982b4928f89","7997debfbaeb45009a2bf4f2e7e487a8","4f50298c14b243409a0f54e2fb6588a7"]},"id":"yv7qsz5d-oUn","outputId":"fcefa6ce-eb3a-49a8-a010-747e2b03be41","execution":{"iopub.status.busy":"2024-05-06T08:44:37.787081Z","iopub.execute_input":"2024-05-06T08:44:37.787373Z","iopub.status.idle":"2024-05-06T09:00:47.185012Z","shell.execute_reply.started":"2024-05-06T08:44:37.787346Z","shell.execute_reply":"2024-05-06T09:00:47.184145Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:cj3xvw0e) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">earthy-dream-14</strong> at: <a href='https://wandb.ai/cs23m032/CS6910_Assignment3/runs/cj3xvw0e' target=\"_blank\">https://wandb.ai/cs23m032/CS6910_Assignment3/runs/cj3xvw0e</a><br/> View project at: <a href='https://wandb.ai/cs23m032/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs23m032/CS6910_Assignment3</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240506_084416-cj3xvw0e/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:cj3xvw0e). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240506_084437-sccyxspy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m032/uncategorized/runs/sccyxspy' target=\"_blank\">electric-glade-16</a></strong> to <a href='https://wandb.ai/cs23m032/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m032/uncategorized' target=\"_blank\">https://wandb.ai/cs23m032/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m032/uncategorized/runs/sccyxspy' target=\"_blank\">https://wandb.ai/cs23m032/uncategorized/runs/sccyxspy</a>"},"metadata":{}},{"name":"stderr","text":"2024-05-06 08:45:01.242541: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-06 08:45:01.242646: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-06 08:45:01.344962: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157cac66472842cd89d5b5b3043e220b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358edc55e9744e489487ea34ee0943fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x1000 with 9 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.036 MB of 0.036 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▃▅▅▆▅▃▆▃▃▂▂▇▂▆▁▅▅▄▇▇▆▄▄▅▄▃▆▄▅▅▄▄▇▅▆▄▇▅▅█</td></tr><tr><td>test_loss</td><td>▆▁▂▂▂▅▁▇▃▂▃▂▅▃▅▃▄▄▂▂▂▅█▃▂▂▂▄▂▁▂▃▂▄▂▄▂▂▂▃</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇█████</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>53.125</td></tr><tr><td>test_loss</td><td>0.42325</td></tr><tr><td>train_accuracy</td><td>55.45703</td></tr><tr><td>train_loss</td><td>0.14424</td></tr><tr><td>val_accuracy</td><td>41.38184</td></tr><tr><td>val_loss</td><td>0.36352</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">electric-glade-16</strong> at: <a href='https://wandb.ai/cs23m032/uncategorized/runs/sccyxspy' target=\"_blank\">https://wandb.ai/cs23m032/uncategorized/runs/sccyxspy</a><br/> View project at: <a href='https://wandb.ai/cs23m032/uncategorized' target=\"_blank\">https://wandb.ai/cs23m032/uncategorized</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240506_084437-sccyxspy/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# import csv\n# rows = []\n# count=0\n# with open(\"/kaggle/working/Output.csv\", 'r') as file:\n#     csvreader = csv.reader(file)\n#     header = next(csvreader)\n#     for row in csvreader:\n#         rows.append(row)\n#         count=count+1\n#         if(count==15):\n#             break\n# print(header)\n# print(rows)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T09:00:47.186218Z","iopub.execute_input":"2024-05-06T09:00:47.186488Z","iopub.status.idle":"2024-05-06T09:00:47.193624Z","shell.execute_reply.started":"2024-05-06T09:00:47.186463Z","shell.execute_reply":"2024-05-06T09:00:47.192729Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"['input', 'target', 'predicted']\n[['thermax', 'थरमैक्स', 'थर्माक्स'], ['sikhaaega', 'सिखाएगा', 'सिखाएगा'], ['learn', 'लर्न', 'लीयर्न'], ['twitters', 'ट्विटर्स', 'ट्विटर्स'], ['tirunelveli', 'तिरुनेलवेली', 'तिरुनेलवेली'], ['independence', 'इंडिपेंडेंस', 'इंडपेंडेंस'], ['speshiyon', 'स्पेशियों', 'स्पेशियों'], ['shurooh', 'शुरूः', 'शुरूह'], ['kolhapur', 'कोल्हापुर', 'कोल्हापुर'], ['ajhar', 'अजहर', 'अझर'], ['karaar', 'क़रार', 'करार'], ['anka', 'अंक', 'अंका'], ['wpd', 'डब्ल्यूपीडी', 'वीपीडी'], ['haashie', 'हाशिए', 'हाशी'], ['glendale', 'ग्लेंडल', 'ग्लेंडाल']]\n","output_type":"stream"}]}]}