{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8190591,"sourceType":"datasetVersion","datasetId":4850432}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport pathlib\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nimport random\nfrom torch.autograd import Variable\nimport time\nimport numpy as np\nimport csv\nimport matplotlib.pyplot as plt\nimport time\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-02T10:13:04.151909Z","iopub.execute_input":"2024-05-02T10:13:04.152814Z","iopub.status.idle":"2024-05-02T10:13:04.159145Z","shell.execute_reply.started":"2024-05-02T10:13:04.152768Z","shell.execute_reply":"2024-05-02T10:13:04.158187Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"class DataProcessing():\n\n    # Constructor for DataProcessing\n    def __init__(self, DATAPATH, source_lang, target_lang,device,config):\n        \n        # Source language\n        self.source_lang = source_lang\n\n        # Target language\n        self.target_lang = target_lang\n        \n        # Start of Word and its Integer representation\n        self.SOW = '>'\n        self.SOW_char2int = 0\n\n        # End of Word and its Integer representation\n        self.EOW = '<'\n        self.EOW_char2int = 1\n\n        # Padding and its Integer representation\n        self.PAD = '.'\n        self.PAD_char2int = 2\n\n        # Unknown and its Integer representation\n        self.UNK = '?'\n        self.UNK_char2int = 3 \n\n        self.device = device\n        self.batch_size = config[\"batch_size\"]\n        \n        # Get path of train,validation and test data\n        self.trainPath = os.path.join(DATAPATH,self.target_lang,self.target_lang + \"_train.csv\")\n        self.validationPath = os.path.join(DATAPATH,self.target_lang,self.target_lang + \"_valid.csv\")\n        self.testPath = os.path.join(DATAPATH,self.target_lang,self.target_lang + \"_test.csv\")\n        \n        # Load train data and set the column names - [source,target]\n        self.train = pd.read_csv(\n            self.trainPath,\n            sep=\",\",\n            names=[\"source\", \"target\"],\n        )\n\n        # Load validation data and set the column names - [source,target]\n        self.val = pd.read_csv(\n            self.validationPath,\n            sep=\",\",\n            names=[\"source\", \"target\"],\n        )\n\n        # Load test data and set the column names - [source,target]\n        self.test = pd.read_csv(\n            self.testPath,\n            sep=\",\",\n            names=[\"source\", \"target\"],\n        )\n        \n        # Creates train data\n        self.train_data = self.preprocess(self.train[\"source\"].to_list(), self.train[\"target\"].to_list())\n\n        # Store encoder input,decoder input,decoder target,source vocabulary,target vocabulary\n        self.trainEncodeInput,self.trainDecoderInput,self.trainDecoderTarget,self.source_vocab,self.target_vocab = self.train_data\n        \n        self.source_char2int,self.source_int2char = self.source_vocab\n        self.target_char2int,self.target_int2char = self.target_vocab\n        self.max_length = self.getMaxLength()\n       \n    def encode(self, source_words, target_words, source_chars, target_chars, source_char2int = None, target_char2int = None):\n        '''\n        Input - 1.source_words - list of all source words\n                2.target_words - list of all target words\n                3.source_chars - sorted list of all characters in source language\n                4.target_chars - sourted list of all characters in target langauge\n                5.source_char2int - Dictionary mappig of charcater to integer for source words\n                6.target_char2int - Dictionary mappig of charcater to integer for target words\n        ''' \n\n        # Generate source and target vocab pairs containing dictionary mapping of character to integer and integer to character for source and target words\n        source_vocab, target_vocab = None, None\n        if source_char2int == None and target_char2int == None:\n\n            source_char2int = dict([(char, i + 4) for i, char in enumerate(source_chars)])\n            target_char2int = dict([(char, i + 4) for i, char in enumerate(target_chars)])\n\n            source_int2char = dict([(i + 4, char) for i, char in enumerate(source_chars)])\n            target_int2char = dict([(i + 4, char) for i, char in enumerate(target_chars)])\n\n            # Add SOW to dictionaries\n            source_char2int[self.SOW] = self.SOW_char2int\n            source_int2char[self.SOW_char2int] = self.SOW\n            target_char2int[self.SOW] = self.SOW_char2int\n            target_int2char[self.SOW_char2int] = self.SOW\n\n            # Add EOW to dictionaries\n            source_char2int[self.EOW] = self.EOW_char2int\n            source_int2char[self.EOW_char2int] = self.EOW\n            target_char2int[self.EOW] = self.EOW_char2int\n            target_int2char[self.EOW_char2int] = self.EOW\n\n            # Add PAD to dictionaries\n            source_char2int[self.PAD] = self.PAD_char2int\n            source_int2char[self.PAD_char2int] = self.PAD\n            target_char2int[self.PAD] = self.PAD_char2int\n            target_int2char[self.PAD_char2int] = self.PAD\n\n            # Add UNK to dictionaries\n            source_char2int[self.UNK] = self.UNK_char2int\n            source_int2char[self.UNK_char2int] = self.UNK\n            target_char2int[self.UNK] = self.UNK_char2int\n            target_int2char[self.UNK_char2int] = self.UNK\n\n            source_vocab = (source_char2int,source_int2char)\n            target_vocab = (target_char2int,target_int2char)\n        \n        \n        self.encoder_input_data = np.zeros((len(source_words), self.max_source_length,self.num_encoder_tokens), dtype=\"float32\")\n        self.decoder_input_data = np.zeros((len(source_words), self.max_target_length,self.num_decoder_tokens), dtype=\"float32\")\n        self.decoder_target_data = np.zeros((len(source_words), self.max_target_length,self.num_decoder_tokens), dtype=\"float32\")\n            \n        \n        if source_vocab != None and target_vocab != None:\n            return (\n                    self.encoder_input_data,\n                    self.decoder_input_data,\n                    self.decoder_target_data,\n                    source_vocab,\n                    target_vocab,\n                )\n        \n        # Source and TargetVocab were not created in the function. \n        # This implies sourceCharToInt and targetCharToInt were not None. Hence the vocab info is already present and we don't return the two tuples.\n        else:\n            return self.encoder_input_data, self.decoder_input_data, self.decoder_target_data\n\n    def preprocess(self, source , target):\n       \n        # Creating list of words used in source language and converting them into string\n        self.source_words = []\n        for src in source:\n            self.source_words.append(str(src))\n        \n        # Creating list of words used in target language and converting them into string\n        self.target_words = []\n        for trg in target:\n            self.target_words.append(str(trg))\n        \n        # Set used to store characters used in source language\n        source_chars = set()\n        \n        # Set used to store characters used in target language\n        target_chars = set()\n\n        # Populate source_chars and target_chars\n        for src, tgt in zip(self.source_words, self.target_words):\n            for char in src:\n                source_chars.add(char)\n\n            for char in tgt:\n                target_chars.add(char)\n\n        \n        # Total number of training samples\n        self.number_of_train_samples = len(self.source_words)\n        \n        \n        # Sort the characters used in source and target language       \n        source_chars = sorted(list(source_chars))\n        target_chars = sorted(list(target_chars))\n\n        # Number of unique characters in source language + Start_of_word,end_of_word,padding and unknown token\n        self.num_encoder_tokens = len(source_chars) + 4\n        \n        # Number of unique characters in target language + Start_of_word,end_of_word,padding and unknown token\n        self.num_decoder_tokens = len(target_chars) + 4\n\n        # Length of maximum word in source_words\n        self.max_source_length = max([len(txt) for txt in self.source_words])\n\n        # Length of maximum word in target_words\n        self.max_target_length = max([len(txt) for txt in self.target_words])\n\n        return self.encode(self.source_words, self.target_words, source_chars, target_chars)\n\n    # Returns list of integer mapped to character\n    def indexesFromWord(self,lang,word):\n        indexes = []\n        if lang == \"source\":\n            for char in word:\n                # If character is in dictionary,add it to the list ,else add unknown token\n                if char in self.source_char2int:\n                    indexes.append(self.source_char2int[char])\n                else:\n                    indexes.append(self.UNK_char2int)\n        if lang == \"target\":\n            for char in word:\n                # If character is in dictionary,add it to the list ,else add unknown token\n                if char in self.target_char2int:\n                    indexes.append(self.target_char2int[char])\n                else:\n                    indexes.append(self.UNK_char2int)\n\n        return indexes\n\n    # Create tensor for word\n    def tensorFromWord(self,lang, word):\n        # Gets list of integer mapped to character\n        indexes = self.indexesFromWord(lang, word)\n        \n        # Append EOW2Int \n        indexes.append(self.EOW_char2int)\n        max_length = self.max_length\n        \n        # Add padding\n        len_padding = max_length - len(indexes) + 1\n        \n        indexes.extend([self.PAD_char2int for i in range(len_padding)])\n        \n        return torch.tensor(indexes, dtype = torch.long, device = self.device).view(-1, 1)\n\n    # Create tensor from pair \n    def tensorsFromPair(self,pairs):\n        # Get the source and target word for a given pair and generate tensors for them\n        source_tensor = self.tensorFromWord(\"source\", pairs[0])\n        target_tensor = self.tensorFromWord(\"target\", pairs[1])\n\n        return (source_tensor, target_tensor)\n    \n    # Create list of pairs conataining source_words and target_words for train data\n    def createTrainPairs(self):\n        pairs  = []\n        source_words = self.source_words\n        target_words = self.target_words\n\n        for source_word,target_word in zip(source_words,target_words):\n            pairs.append((source_word,target_word))\n        return pairs\n    \n    # Create list of pairs conataining source_words and target_words for validation data\n    def createValidationData(self):\n        pairs = []\n        source_words = []\n        target_words = []\n        for word in self.val[\"source\"].to_list():\n            source_words.append(word)\n\n        for word in self.val[\"target\"].to_list():\n            target_words.append(word)\n        \n        for source_word,target_word in zip(source_words,target_words):\n            pairs.append((source_word,target_word))\n\n        return pairs\n    \n    # Create list of pairs conataining source_words and target_words for test data\n    def createTestData(self):\n        pairs = []\n        source_words = []\n        target_words = []\n        for word in self.test[\"source\"].to_list():\n            source_words.append(word)\n\n        for word in self.test[\"target\"].to_list():\n            target_words.append(word)\n        \n        for source_word,target_word in zip(source_words,target_words):\n            pairs.append((source_word,target_word))\n\n        return pairs\n    \n    # Returns the maximum length of word from source_lang and target_lang\n    def getMaxLength(self):\n        \n        source_words = []\n        target_words = []\n        for word in self.val[\"source\"].to_list():\n            source_words.append(word)\n\n        for word in self.val[\"target\"].to_list():\n            target_words.append(word)\n        \n        val_max_source_length = max([len(txt) for txt in source_words])\n        val_max_target_length = max([len(txt) for txt in target_words])\n        \n        source_words = []\n        target_words = []\n        for word in self.test[\"source\"].to_list():\n            source_words.append(word)\n\n        for word in self.test[\"target\"].to_list():\n            target_words.append(word)\n        \n        test_max_source_length = max([len(txt) for txt in source_words])\n        test_max_target_length = max([len(txt) for txt in target_words])\n\n        self.maxSourceLength = max([self.max_source_length,val_max_source_length,test_max_source_length])\n        self.maxTargetLength = max([self.max_target_length,val_max_target_length,test_max_target_length])\n\n        return max(self.maxSourceLength,self.maxTargetLength)\n    \n    # Returns loader for validation data\n    def getValLoader(self):\n        \n        validationPairs = self.createValidationData()\n        validation_pairs = []\n        for pair in validationPairs:\n            validation_pairs.append(self.tensorsFromPair(pair))\n        \n        val_Loader = DataLoader(validation_pairs,batch_size = self.batch_size,shuffle = True)\n        return val_Loader\n    \n    # Returs loader for test data\n    def getTestLoader(self):\n        testPairs = self.createTestData()\n\n        test_pairs = []\n        for pair in testPairs:\n            test_pairs.append(self.tensorsFromPair(pair))\n        \n        test_Loader = DataLoader(test_pairs,batch_size = self.batch_size,shuffle = True)\n\n        return test_Loader\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:39:22.749428Z","iopub.execute_input":"2024-05-02T09:39:22.749820Z","iopub.status.idle":"2024-05-02T09:39:22.798003Z","shell.execute_reply.started":"2024-05-02T09:39:22.749790Z","shell.execute_reply":"2024-05-02T09:39:22.797111Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    # Encoder Destructor\n    def __init__(self, input_size,config):\n        super(Encoder, self).__init__()\n\n        # Store parameters in class varaibles\n        self.hidden_size = config[\"hidden_size\"]\n        self.embedding_size = config[\"embedding_size\"]\n        self.cell_type = config[\"cell_type\"] \n        self.numLayers = config[\"numLayers\"]\n        self.drop_out = config[\"drop_out\"]\n        self.bidirectional = config[\"bidirectional\"]\n        self.batch_size = config[\"batch_size\"]\n        \n        # input_size - contains the number of encoder tokens which is input to Embedding\n        # hidden_size - size of each embedding vector\n        # Create an Embedding for the Input \n        # Each character will have an embedding of size = embedding_size\n        self.embedding = nn.Embedding(input_size, self.embedding_size)\n        self.dropout = nn.Dropout(self.drop_out)\n        \n        # the cell_type - GRU\n        if self.cell_type == \"gru\":\n            ''' Input to GRU -  1.number of expected features in x - embedded input \n                                2.number of features in hidden state - hidden_size\n                                3.number of layers (stacking GRUs together) '''\n            self.gru = nn.GRU(self.embedding_size, self.hidden_size,num_layers = self.numLayers,dropout = self.drop_out,bidirectional = self.bidirectional)\n            self.rnnLayer = self.gru\n        \n        # the cell_type - RNN\n        if self.cell_type == \"rnn\":\n            ''' Input to RNN -  1.number of expected features in x\n                                2.number of features in hidden state\n                                3.number of layers (stacking RNNs together) '''\n            \n            self.rnn = nn.RNN(self.embedding_size,self.hidden_size,num_layers = self.numLayers,dropout = self.drop_out,bidirectional = self.bidirectional)\n            self.rnnLayer = self.rnn\n        \n        # the cell_type - LSTM\n        if self.cell_type == \"lstm\":\n            ''' Input to LSTM - 1.number of expected features in x\n                                2.number of features in hidden state\n                                3.number of layers (stacking LSTMs together) '''\n            \n            self.lstm = nn.LSTM(self.embedding_size,self.hidden_size,num_layers = self.numLayers,dropout = self.drop_out,bidirectional = self.bidirectional)\n            self.rnnLayer = self.lstm\n\n    # Encoder forward pass\n    def forward(self, input, hidden,cell_state = None):\n        '''Input -> hidden      - initial hidden state for each element in the input sequence\n                    cell_hidden - the initial cell state for each element in the input sequence\n        '''\n       \n        # Creates a embedded tensor by passing the input to the embedding layer and resizing the output to (1,batch_size,-1)\n        embedded = self.dropout(self.embedding(input).view(1, self.batch_size, -1))\n        \n        # Pass this embedded input to the GRU/LSTM/RNN model\n        output = embedded\n          \n        '''Output     -     1.Output features from the last layer\n                            2.final hidden state for each element which is passed to decoder as a context vector'''\n        output, hidden = self.rnnLayer(output, hidden)\n        return output, hidden\n        \n        \n    # Initailizes initial hidden layer for encoder\n    def initHidden(self,device,numLayers):\n        if self.bidirectional:\n            return torch.zeros(numLayers * 2, self.batch_size, self.hidden_size, device=device)\n        else:\n            return torch.zeros(numLayers, self.batch_size, self.hidden_size, device=device)\n\nclass Decoder(nn.Module):\n\n    # Decoder Constructor\n    def __init__(self,output_size,config,data): \n        super(Decoder, self).__init__()\n\n        \n        # Store parameters in class varaibles\n        self.numLayers = config[\"numLayers\"]\n        self.cell_type = config[\"cell_type\"]\n        self.hidden_size = config[\"hidden_size\"]\n        self.embedding_size = config[\"embedding_size\"]\n        # Create embedding for input\n        self.embedding = nn.Embedding(output_size, self.embedding_size)\n        self.drop_out = config[\"drop_out\"]       \n        self.bidirectional = config[\"bidirectional\"]\n        self.batch_size = config[\"batch_size\"]\n        self.dropout = nn.Dropout(self.drop_out)\n        \n        if self.cell_type == \"gru\":\n            self.gru = nn.GRU(self.embedding_size, self.hidden_size,num_layers = self.numLayers,dropout = self.drop_out,bidirectional = self.bidirectional)\n            self.rnnLayer = self.gru\n\n        if self.cell_type == \"rnn\":\n            self.rnn = nn.RNN(self.embedding_size,self.hidden_size,self.numLayers,dropout = self.drop_out,bidirectional = self.bidirectional)\n            self.rnnLayer = self.rnn\n        \n        if self.cell_type == \"lstm\":\n            self.lstm = nn.LSTM(self.embedding_size,self.hidden_size,self.numLayers,dropout = self.drop_out,bidirectional = self.bidirectional)\n            self.rnnLayer = self.lstm\n       \n        # Creating a dense layer\n        if self.bidirectional:\n            self.out = nn.Linear(self.hidden_size * 2 ,output_size)\n        else:\n            self.out = nn.Linear(self.hidden_size, output_size)\n        \n        # Softmax function as an output function\n        self.softmax = nn.LogSoftmax(dim = 1)\n\n    def forward(self, input, hidden,cell_state = None):\n\n        # Create an embedding for input and resize it to (1,batch_size,-1)\n        # The output of embedding layer is passed as an input to decoder\n        output = self.dropout(self.embedding(input).view(1, self.batch_size, -1))\n\n        # Applying ReLU activation function\n        output = F.relu(output)\n\n        # Pass output and previous hidden state to model RNN/LSTM/GRU\n        output, hidden = self.rnnLayer(output, hidden)\n        \n        # apply softmax function as an output function\n        output = self.softmax(self.out(output[0]))\n        \n        return output, hidden\n ","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:39:22.799960Z","iopub.execute_input":"2024-05-02T09:39:22.800438Z","iopub.status.idle":"2024-05-02T09:39:22.824455Z","shell.execute_reply.started":"2024-05-02T09:39:22.800405Z","shell.execute_reply":"2024-05-02T09:39:22.823698Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"teacher_forcing_ratio = 0.5","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:39:22.825624Z","iopub.execute_input":"2024-05-02T09:39:22.825955Z","iopub.status.idle":"2024-05-02T09:39:22.842345Z","shell.execute_reply.started":"2024-05-02T09:39:22.825925Z","shell.execute_reply":"2024-05-02T09:39:22.841519Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"'''\n    Input ->    1.source_tensor - a tensor for given source word containing character indexes\n                2.target_tensor - a tensor for given target word containing character indexes\n                3.encoder - object of Encdoer class\n                4.decoder - object of Decoder class\n                5.encoder_optimizer - optimizer used for encoder\n                6.decoder_optimizer - optimizer used for decoder\n                7.criterion - loss function\n                8.max_length - maximum length of source word\n                9.device - CUDA or CPU\n'''\ndef trainforOneEpoch(config,data,source_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n    \n    device = config[\"device\"]\n    max_length = data.max_length\n    batch_size = config[\"batch_size\"]\n    attention = config[\"attention\"]\n    hidden_size = config[\"hidden_size\"]\n    \n    \n    # Initailize initial hidden layer for encoder\n    encoder_hidden = encoder.initHidden(device,config[\"numLayers\"])\n    \n    if config[\"cell_type\"] == \"lstm\":\n        encoder_cell_state = encoder.initHidden(device,config[\"numLayers\"])\n        encoder_hidden = (encoder_hidden,encoder_cell_state)\n    \n    # Empty the gradients for encoder and decoder\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    source_tensor = source_tensor.squeeze()\n    target_tensor = target_tensor.squeeze()\n    \n    # Length of source and target tensor\n    source_length = source_tensor.size(0)\n    target_length = target_tensor.size(0)\n    \n    \n    # Stores all encoder outputs for each character in source word\n    if attention:\n        encoder_outputs = torch.zeros(max_length + 1,batch_size, hidden_size, device = device)\n\n    # Initialize loss to ZERO\n    loss = 0\n    \n    # encoder encodes each character index in source_word\n    for ei in range(source_length):\n        encoder_output, encoder_hidden = encoder(source_tensor[ei], encoder_hidden)\n        if attention:\n            encoder_outputs[ei] = encoder_output[0]\n    \n\n    # Initialize decoder input with start of word token\n    decoder_input = torch.tensor([[data.SOW_char2int] * batch_size],device = device)\n\n    # initial hidden layer for decoder will be final hidden layer from the encoder\n    decoder_hidden = encoder_hidden\n    \n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n    total_correct_words = 0  # Initialize total correct words //added\n\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            if attention:  \n                decoder_output, decoder_hidden,decoder_attention = decoder(decoder_input, decoder_hidden,encoder_outputs.reshape(batch_size,max_length + 1,hidden_size))\n            else:\n                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n\n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            if attention:\n                decoder_output, decoder_hidden,decoder_attention = decoder(decoder_input, decoder_hidden,encoder_outputs.reshape(batch_size,max_length + 1,hidden_size))\n            else:\n                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            \n            # returns top value and index from the decoder output\n            topv, topi = decoder_output.topk(1)\n            \n            # Squeeze all the dimensions that are 1 and returns a new tensor detatched from the current history graph\n            decoder_input = topi.squeeze().detach()            \n\n            # Compute loss\n            loss += criterion(decoder_output, target_tensor[di])\n             # Compute number of correct words\n            if torch.equal(decoder_input, target_tensor[di]): #added\n                total_correct_words += 1\n\n    # Backpropagation\n    loss.backward()\n\n    # Update parameters\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    # return the loss\n    return loss.item() / target_length , total_correct_words #added\n\n'''\n    Input - 1.object of Encoder class\n            2.object of Decoder class\n            3.n_iters - number of epochs\n            4.print_every - prints every given milliseconds\n            5.plot_every - plots every given milliseconds\n            6.learning rate\n'''\ndef trainIters(config,total_batches,loader,data,encoder,decoder,wandbapply):\n\n    if wandbapply:\n        wandb.init(\n            project=config[\"wandb_project\"]\n        )\n\n    epochs = config[\"epoch\"]\n    learning_rate = config[\"learning_rate\"]\n    criterion = nn.CrossEntropyLoss()\n    \n    # Set optimizers for encoder and decoder\n    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=learning_rate)\n    \n    \n    for epoch in range(epochs):\n        epoch_loss = 0\n        batch_no = 1\n        # Train for each batch\n        epoch_total_correct_words = 0  # Initialize total correct words for the epoch added\n        #tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        for batchx,batchy in tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n               \n            batchx = batchx.transpose(0,1)\n            batchy = batchy.transpose(0,1)\n            batch_loss,batch_correct_words = trainforOneEpoch(config = config,\n                                            data = data,\n                                            source_tensor = batchx, \n                                            target_tensor = batchy, \n                                            encoder = encoder,\n                                            decoder = decoder,\n                                            encoder_optimizer = encoder_optimizer, \n                                            decoder_optimizer = decoder_optimizer, \n                                            criterion = criterion)\n            \n            epoch_loss += batch_loss\n            epoch_total_correct_words += batch_correct_words  # Accumulate total correct words added\n            batch_no+=1\n#             if batch_no % 100 == 0:\n#                 print(\"epoch:\" + str(epoch + 1) + \" / \" + str(epochs) + \"    batch:\" + str(batch_no) + \" / \" + str(total_batches))\n        \n        val_loader = data.getValLoader()\n        \n        if epoch == (epochs - 1):\n            \n            # Compute validation accuracy\n            validation_loss ,validation_accuracy = evaluate(config=config,\n                loader=val_loader,\n                data=data,\n                encoder=encoder,\n                decoder=decoder,\n                training_completed=True,\n                test = False\n            )\n        else:\n            # Compute validation accuracy\n            validation_loss ,validation_accuracy = evaluate(config=config,\n                loader=val_loader,\n                data=data,\n                encoder=encoder,\n                decoder=decoder,\n                training_completed=False,\n                test = False\n            )\n        train_loss = epoch_loss / total_batches\n        train_accuracy = (epoch_total_correct_words / len(loader.dataset)) * 100  # Calculate training accuracy added\n       \n        '''print(\"epoch:{epoch}, train loss:{train_l}, validation loss:{validation_l}, validation accuracy:{validation_ac}\".\\\n                  format(epoch = epoch + 1,train_l = train_loss,validation_l = validation_loss,validation_ac = validation_accuracy))'''\n        \n        print(\"epoch:{epoch}, train loss:{train_l}, train accuracy:{train_ac}, validation loss:{validation_l}, validation accuracy:{validation_ac}\". \\\n              format(epoch=epoch + 1, train_l=train_loss, train_ac=train_accuracy, validation_l=validation_loss,\n                     validation_ac=validation_accuracy))\n        if wandbapply:\n            wandb.log({'train loss':train_loss,'validation loss':validation_loss, 'validation accuracy':validation_accuracy})\n    '''\n    Code for using test data\n    '''\n    print(\"epochs completed\")\n    test_loader = data.getTestLoader()\n        \n    config[\"batch_size\"] = 1\n    test_loss ,test_accuracy = evaluate(config=config,\n             loader=test_loader,\n             data=data,\n             encoder=encoder,\n             decoder=decoder,\n             training_completed=False,\n             test = True\n        )\n    print(\"Test Accuracy:\",test_accuracy)\n    # wandb.init(\n    #         project=config[\"wandb_project\"]\n    #     )\n    # wandb.log({'test loss':test_loss,'test accuracy':test_accuracy})\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T10:35:21.780277Z","iopub.execute_input":"2024-05-02T10:35:21.780647Z","iopub.status.idle":"2024-05-02T10:35:21.809485Z","shell.execute_reply.started":"2024-05-02T10:35:21.780616Z","shell.execute_reply":"2024-05-02T10:35:21.808451Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"def evaluate(config,data, loader, encoder, decoder,training_completed,test) :\n\n        loss = 0\n        totalCorrectWords = 0\n        batchNumber = 1\n        batch_size = config[\"batch_size\"]\n        \n        totalWords = len(loader.sampler)\n        totalBatches = len(loader.sampler) // batch_size\n\n        # Loss Function\n        criterion = nn.CrossEntropyLoss()\n\n        for sourceTensor, targetTensor in loader :\n            batchLoss, correctWords,attentions = evaluateOneBatch(config,data,sourceTensor, targetTensor, encoder, decoder, criterion,test)\n\n            loss += batchLoss\n            totalCorrectWords += correctWords\n\n        # If training is completed,then dispay heatmaps\n        if training_completed == True :\n            if attentions is not None:\n                volume  = attentions.numpy()\n\n                for point in range(10):\n                    heatMap = np.zeros((data.max_length + 1,data.max_length + 1))\n\n                    for i in range(data.max_length + 1):\n                        for k in range(data.max_length + 1):\n                            heatMap[i][k] = volume[i][point][k]\n\n\n                    # plotHeatMap(heatMap)\n\n        return (loss / totalBatches), (totalCorrectWords / totalWords) * 100\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T10:34:33.205533Z","iopub.execute_input":"2024-05-02T10:34:33.206234Z","iopub.status.idle":"2024-05-02T10:34:33.215217Z","shell.execute_reply.started":"2024-05-02T10:34:33.206201Z","shell.execute_reply":"2024-05-02T10:34:33.214101Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"def evaluateOneBatch(config,data, sourceTensorBatch, targetTensorBatch, encoder, decoder, criterion,test) :\n\n        loss = 0\n        correctWords = 0\n\n        batchSize = data.batch_size\n        device = config[\"device\"]\n        maxLengthWord = data.max_length\n        cell_type = config[\"cell_type\"]\n        attention = config[\"attention\"]\n        hidden_size = config[\"hidden_size\"]\n\n        sourceTensor = Variable(sourceTensorBatch.transpose(0, 1))\n        targetTensor = Variable(targetTensorBatch.transpose(0, 1))\n        \n        # Get source length\n        sourceTensorLength = sourceTensor.size()[0]\n        targetTensorLength = targetTensor.size()[0]\n\n        predictedBatchOutput = torch.zeros(targetTensorLength, batchSize, device = device)\n\n        # Initialize initial hidden state of encoder\n        encoderHidden = encoder.initHidden(device = device,numLayers = config[\"numLayers\"])\n\n        if cell_type == \"lstm\":\n            encoderCell = encoder.initHidden(device = device,numLayers = config[\"numLayers\"])\n            encoderHidden = (encoderHidden, encoderCell)\n\n        if attention:\n            encoderOutputs = torch.zeros(maxLengthWord + 1, batchSize, hidden_size, device = device)\n\n        for ei in range(sourceTensorLength):\n            encoderOutput, encoderHidden = encoder(sourceTensor[ei], encoderHidden)\n\n            if attention :\n                encoderOutputs[ei] = encoderOutput[0]\n\n        # Initialize input to decoder with start of word token\n        decoderInput = torch.tensor([[data.SOW_char2int] * batchSize], device = device)\n\n        # initial hidden state for decoder will be final hidden state of encoder\n        decoderHidden = encoderHidden\n\n        if attention :\n            decoderAttentions = torch.zeros(maxLengthWord + 1,batchSize, maxLengthWord + 1)\n        \n        for di in range(targetTensorLength):\n            if attention :\n                # Pass the decoderInput, decoderHidden and encoderOutputs to the decoder\n                decoderOutput, decoderHidden, decoderAttention = decoder(decoderInput, decoderHidden, encoderOutputs.reshape(batchSize,maxLengthWord + 1,hidden_size))\n                decoderAttentions[di] = decoderAttention.data\n            else : \n                decoderOutput, decoderHidden = decoder(decoderInput, decoderHidden)\n            \n            loss += criterion(decoderOutput, targetTensor[di].squeeze())\n            \n            topv, topi = decoderOutput.data.topk(1)\n            decoderInput = torch.cat(tuple(topi))\n            predictedBatchOutput[di] = torch.cat(tuple(topi))\n#             if torch.equal(topi.squeeze().detach(), targetTensor[di].squeeze()): #added\n#                 correctWords += 1\n    \n        predictedBatchOutput = predictedBatchOutput.transpose(0,1)\n\n        ignore = [data.SOW_char2int, data.EOW_char2int,data.PAD_char2int]\n        \n        predicted_list = []\n        target_list = []\n        input_list = []\n        \n        for di in range(predictedBatchOutput.size()[0]):\n\n            predicted = [letter.item() for letter in predictedBatchOutput[di] if letter not in ignore]\n            actual = [letter.item() for letter in targetTensorBatch[di] if letter not in ignore]\n            inputText = [letter.item() for letter in sourceTensorBatch[di] if letter not in ignore]\n\n            predictedChars = [data.target_int2char[char] for char in predicted]\n            actualChars = [data.target_int2char[char] for char in actual]\n            inputChars = [data.source_int2char[char] for char in inputText]\n\n            predictedWord = \"\".join([str(i) for i in predictedChars])\n            actualWord = \"\".join([str(i) for i in actualChars])\n            inputWord = \"\".join([str(i) for i in inputChars])\n\n            predicted_list.append(predictedWord)\n            target_list.append(actualWord)\n            input_list.append(inputWord)\n\n            if predicted == actual:\n                correctWords += 1\n        \n        if test:\n            writeToCSV(predicted_list,target_list,input_list)\n                \n        if attention:\n            return loss.item() / len(sourceTensorBatch), correctWords,decoderAttentions\n        \n        return loss.item() / len(sourceTensorBatch), correctWords,None\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:39:22.886723Z","iopub.execute_input":"2024-05-02T09:39:22.887040Z","iopub.status.idle":"2024-05-02T09:39:22.906306Z","shell.execute_reply.started":"2024-05-02T09:39:22.887009Z","shell.execute_reply":"2024-05-02T09:39:22.905272Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nRun this file if you want to run the code without WANDB\n'''\n\ndef trainForConfigs(config,add_wandb):\n    # Load and pre-process data\n    device = config[\"device\"]\n    data = DataProcessing(DATAPATH = '/kaggle/input/aksharantar-sampled/aksharantar_sampled', source_lang = config[\"source_lang\"], target_lang = config[\"target_lang\"],device = device,config = config)\n    \n    config[\"maxLength\"] = data.getMaxLength()\n    batch_size = config[\"batch_size\"]\n    \n    # Create encoder with input size = number of characters in source langauge and specified embedding size\n    encoder = Encoder(data.num_encoder_tokens,config).to(device)\n    \n    # Create encoder with output size = number of characters in target langauge and specified embedding size\n    decoder = Decoder(data.num_decoder_tokens,config,data).to(device)\n    \n\n    trainLoader,total_batches = getTrainLoader(data,batch_size)    \n    \n    # Train the model and compute loss and accuracy\n    trainIters(config = config,loader=trainLoader,total_batches=total_batches,data = data,encoder = encoder,decoder = decoder,wandbapply = add_wandb)\n    \n# Returns loader for train data and total number of batches in training data\ndef getTrainLoader(data,batch_size):\n        trainPairs = data.createTrainPairs()\n        training_pairs = []\n        for pair in trainPairs:\n            training_pairs.append(data.tensorsFromPair(pair))\n        \n        trainLoader = DataLoader(training_pairs,batch_size = batch_size,shuffle = True)\n        total_batches = len(training_pairs) // batch_size\n   \n        return trainLoader,total_batches\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:39:22.933881Z","iopub.execute_input":"2024-05-02T09:39:22.934544Z","iopub.status.idle":"2024-05-02T09:39:22.942483Z","shell.execute_reply.started":"2024-05-02T09:39:22.934519Z","shell.execute_reply":"2024-05-02T09:39:22.941539Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    config = {\n        \"wandb_project\": \"DL Assignment 3-1\",\n        \"wandb_entity\": \"cs22m019\",\n        \"hidden_size\" : 512,\n        \"source_lang\" : 'en',\n        \"target_lang\" : 'hin',\n        \"cell_type\"   : \"gru\",\n        \"numLayers\" : 2,\n        \"drop_out\"    : 0.3, \n        \"embedding_size\" : 256,\n        \"bidirectional\" : False,\n        \"batch_size\" : 32,\n        \"attention\" : False,\n        \"epoch\" : 5,\n        \"device\" : device,\n        \"learning_rate\" : 0.001 #not good for 0.01\n    }\n\n    # Update parameters obtained from command line\n#     update_parameters(config)\n    \n    startime = time.time()\n    trainForConfigs(config,add_wandb=False)\n    endTime = (time.time() - startime)\n    print(endTime / 60)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T10:35:27.451648Z","iopub.execute_input":"2024-05-02T10:35:27.452442Z","iopub.status.idle":"2024-05-02T10:46:34.651008Z","shell.execute_reply.started":"2024-05-02T10:35:27.452408Z","shell.execute_reply":"2024-05-02T10:46:34.649749Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1600/1600 [01:38<00:00, 16.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch:1, train loss:0.5987209287727332, train accuracy:18.32421875, validation loss:0.3879552190192044, validation accuracy:17.9443359375\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1600/1600 [01:38<00:00, 16.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch:2, train loss:0.3873124825733679, train accuracy:18.2421875, validation loss:0.3116851083468646, validation accuracy:21.240234375\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1600/1600 [01:39<00:00, 16.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch:3, train loss:0.3534393382072445, train accuracy:18.49609375, validation loss:0.30491670151241124, validation accuracy:21.0205078125\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1600/1600 [01:39<00:00, 16.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch:4, train loss:0.33201204475981266, train accuracy:18.203125, validation loss:0.3018288140883669, validation accuracy:25.0\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1600/1600 [01:39<00:00, 16.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch:5, train loss:0.3176198307949081, train accuracy:17.873046875, validation loss:0.31070759252179414, validation accuracy:24.951171875\nepochs completed\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[90], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Update parameters obtained from command line\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     update_parameters(config)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m startime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrainForConfigs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43madd_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m endTime \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m startime)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(endTime \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m)\n","Cell \u001b[0;32mIn[66], line 23\u001b[0m, in \u001b[0;36mtrainForConfigs\u001b[0;34m(config, add_wandb)\u001b[0m\n\u001b[1;32m     20\u001b[0m trainLoader,total_batches \u001b[38;5;241m=\u001b[39m getTrainLoader(data,batch_size)    \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model and compute loss and accuracy\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtotal_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwandbapply\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madd_wandb\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[89], line 195\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(config, total_batches, loader, data, encoder, decoder, wandbapply)\u001b[0m\n\u001b[1;32m    192\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgetTestLoader()\n\u001b[1;32m    194\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 195\u001b[0m test_loss ,test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m         \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m         \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m         \u001b[49m\u001b[43mtraining_completed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m         \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m,test_accuracy)\n","Cell \u001b[0;32mIn[86], line 15\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(config, data, loader, encoder, decoder, training_completed, test)\u001b[0m\n\u001b[1;32m     12\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sourceTensor, targetTensor \u001b[38;5;129;01min\u001b[39;00m loader :\n\u001b[0;32m---> 15\u001b[0m     batchLoss, correctWords,attentions \u001b[38;5;241m=\u001b[39m \u001b[43mevaluateOneBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43msourceTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batchLoss\n\u001b[1;32m     18\u001b[0m     totalCorrectWords \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m correctWords\n","Cell \u001b[0;32mIn[65], line 96\u001b[0m, in \u001b[0;36mevaluateOneBatch\u001b[0;34m(config, data, sourceTensorBatch, targetTensorBatch, encoder, decoder, criterion, test)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#         if torch.equal(decoderInput, targetTensorBatch[di]): #added\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m#                 correctWords += 1\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m test:\n\u001b[0;32m---> 96\u001b[0m             \u001b[43mwriteToCSV\u001b[49m(predicted_list,target_list,input_list)\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m attention:\n\u001b[1;32m     99\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(sourceTensorBatch), correctWords,decoderAttentions\n","\u001b[0;31mNameError\u001b[0m: name 'writeToCSV' is not defined"],"ename":"NameError","evalue":"name 'writeToCSV' is not defined","output_type":"error"}]}]}